[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data Code & Coffee",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\n\nqbr v1.0.0\n\n\nFeb 9, 2022\n\n\n\n\n\n\n\nOh {drat}!\n\n\nOct 31, 2021\n\n\n\n\n\n\n\nIntroducing {qbr}.\n\n\nJun 20, 2021\n\n\n\n\n\n\n\n{dm} is fantastic!\n\n\nMay 9, 2021\n\n\n\n\n\n\n\nZero to consciously incompetent with {Rcpp}.\n\n\nApr 4, 2021\n\n\n\n\n\n\n\nMany models, many plots, many pages!\n\n\nJan 29, 2021\n\n\n\n\n\n\n\nMusings on the RStudio instructor training process\n\n\nJan 17, 2021\n\n\n\n\n\n\n\nHow has Covid-19 affected UK athletics rankings in 2020?\n\n\nDec 26, 2020\n\n\n\n\n\n\n\nWhen one model is not enough: Stacking models with {stacks}.\n\n\nNov 30, 2020\n\n\n\n\n\n\n\n{bitmexr} gets a hex logo!\n\n\nNov 22, 2020\n\n\n\n\n\n\n\nInvestigating sports injuries with Bayesian networks using {bnlearn}.\n\n\nNov 17, 2020\n\n\n\n\n\n\n\nIntroducing {poweRof10}.\n\n\nOct 24, 2020\n\n\n\n\n\n\n\nBuild with R, deploy with Python (and Heroku).\n\n\nJul 14, 2020\n\n\n\n\n\n\n\nPenguins and nakedpipes\n\n\nJun 21, 2020\n\n\n\n\n\n\n\nOpening the black box: Exploring xgboost models with {fastshap} in R\n\n\nJun 7, 2020\n\n\n\n\n\n\n\nbitmexr 0.3.0: Place, modify and cancel your orders on BitMEX without leaving R!\n\n\nMay 25, 2020\n\n\n\n\n\n\n\n{tidymodels} workflow with Bayesian optimisation\n\n\nMay 23, 2020\n\n\n\n\n\n\n\nPretty tables with {gt}.\n\n\nMay 2, 2020\n\n\n\n\n\n\n\nbitmexr: An R client for BitMEX cryptocurrency exchange.\n\n\nApr 13, 2020\n\n\n\n\n\n\n\nExploring the recent Bitcoin crash with {tidyquant} and {gganimate}.\n\n\nMar 22, 2020\n\n\n\n\n\n\n\nWriting a thesis in R Markdown\n\n\nMar 14, 2020\n\n\n\n\n\n\n\nAthletics rankings\n\n\nMar 8, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Code & Coffee",
    "section": "",
    "text": "I have a my blog that is infrequently updated üìù\nSee my projects for some things I‚Äôve been working üõ†\nFeel free to get in contact below"
  },
  {
    "objectID": "posts/2020-11-01-bayesian-networks-with-bnlearn/index.html",
    "href": "posts/2020-11-01-bayesian-networks-with-bnlearn/index.html",
    "title": "Investigating sports injuries with Bayesian networks using {bnlearn}.",
    "section": "",
    "text": "In this post I want to share a few details about the humble Bayesian Network (BN). While not generally considered one of the ‚Äúmainstream‚Äù forms of analysis, Bayesian networks have several features that make them an interesting choice, including their flexibility, interpretability and visually appealing nature. I used BNs during PhD, and was fortunate enough to receive some guidance from Marco Scutari, the author of bnlearn, and who is a leading expert in the field when it comes to all things BN. From the outset, I want to clarify that I am by no means an expert in Bayesian networks (far from it!), and encourage any reader who wants to learn more about the theory underlying BNs, or to see more in depth examples, to visit Marco‚Äôs site which is a fantastic place to start. I have also included a list of resources I found helpful when learning about BNs at the end of the post. That said, in this post I hope to show a motivating example that might spark some ideas about how BNs can be used!"
  },
  {
    "objectID": "posts/2020-11-01-bayesian-networks-with-bnlearn/index.html#resources",
    "href": "posts/2020-11-01-bayesian-networks-with-bnlearn/index.html#resources",
    "title": "Investigating sports injuries with Bayesian networks using {bnlearn}.",
    "section": "Resources",
    "text": "Resources\n\n\nhttps://www.bnlearn.com/ - Marco‚Äôs / bnlearn website\nhttps://kevintshoemaker.github.io/NRES-746/BayesianNetworks-1.html - a great blog post about BNs\nhttps://bookdown.org/robertness/causalml/docs/tutorial-probabilistic-modeling-with-bayesian-networks-and-bnlearn.html another good starting point covering the basics\nhttps://www.frontiersin.org/articles/10.3389/fvets.2020.00073/full Kratzer et al., (2020) paper I found while writing this post that gives a great walkthough, although not using bnlearn."
  },
  {
    "objectID": "posts/2020-06-21-penguins-and-nakedpipes/index.html",
    "href": "posts/2020-06-21-penguins-and-nakedpipes/index.html",
    "title": "Penguins and nakedpipes",
    "section": "",
    "text": "So what does nakedpipe do?\nI‚Äôm sure most R users are familiar with magrittr‚Äôs pipe (%>%) operator. The %>% allows you to chain together multiple commands by piping forward an expression into a function like x %>% f, rather than f(x) Bache and Wickham (2014). The resulting code is usually very readable and easy to debug. nakedpipe adopts a similar style, but removes the need to use %>% after every function, and adds some additional logging/debugging features as well as being slightly faster than the magrittr implementation.\nIn order to try out the features in nakedpipe I used the new palmerpenguins dataset (KB, TD, and WR 2014) which provides a great test dataset and is a refreshing alternative the usual suspects, mtcars et al‚Ä¶\n\nGetting started\n\n#remotes::install_github(\"allisonhorst/palmerpenguins\")\n#remotes::install_github(\"moodymudskipper/nakedpipe\")\n\nlibrary(tidyverse)\nlibrary(palmerpenguins) \nlibrary(nakedpipe)\nlibrary(magrittr)\n\npenguins <- penguins\n\nA basic example looks like this:\n\npenguins %.% {\n  filter(sex == \"female\") \n  select(1:5) \n  head(5)\n}   \n\n# A tibble: 5 √ó 5\n  species island    bill_length_mm bill_depth_mm flipper_length_mm\n  <fct>   <fct>              <dbl>         <dbl>             <int>\n1 Adelie  Torgersen           39.5          17.4               186\n2 Adelie  Torgersen           40.3          18                 195\n3 Adelie  Torgersen           36.7          19.3               193\n4 Adelie  Torgersen           38.9          17.8               181\n5 Adelie  Torgersen           41.1          17.6               182\n\n\nYou use the %.% operator to ‚Äúpipe‚Äù into a sequence of functions, which are within {}. You can reduce the code further by removing the subset / filter argument like:\n\n\nAny call to < > <= >= == != %in% & | will work in this way\n\npenguins %.% {\n  sex == \"female\"\n  select(1:5) \n  head(5)\n}  \n\n# A tibble: 5 √ó 5\n  species island    bill_length_mm bill_depth_mm flipper_length_mm\n  <fct>   <fct>              <dbl>         <dbl>             <int>\n1 Adelie  Torgersen           39.5          17.4               186\n2 Adelie  Torgersen           40.3          18                 195\n3 Adelie  Torgersen           36.7          19.3               193\n4 Adelie  Torgersen           38.9          17.8               181\n5 Adelie  Torgersen           41.1          17.6               182\n\n\nSimilarly, this works with transform / mutate calls. The use of the = sign implies a new column is being created, for example:\n\npenguins %.% {\n  sex == \"female\"\n  body_mass_kg = body_mass_g/1000\n  select(body_mass_g, body_mass_kg)\n  head(5)\n}  \n\n  body_mass_g body_mass_kg\n1        3800        3.800\n2        3250        3.250\n3        3450        3.450\n4        3625        3.625\n5        3200        3.200\n\n\nAssigning the result of a sequence of function can be done by using -> at the end of the sequence (outside of the {}) or using the %<.% operator at the start of the sequence.\n\npenguins %.% {\n  sex == \"female\"\n  arrange(body_mass_g)\n  select(1,2,6) \n  head(5)\n} -> small_penguins\n\n# Or\n\nsmall_penguins <- penguins\n\nsmall_penguins %<.% {\n  sex == \"female\"\n  arrange(body_mass_g)\n  select(1,2,6) \n  head(5)\n} \n\nAdditionally, you can create outputs midway through the sequence of functions by using ~~:\n\npenguins %.% {\n  sex == \"female\"\n  ~~ . -> female_penguins\n  select(1:5) \n  head(5) \n} -> x\n\nhead(female_penguins)\n\n# A tibble: 6 √ó 8\n  species island bill_length_mm bill_depth_mm flipper_length_‚Ä¶ body_mass_g sex  \n  <fct>   <fct>           <dbl>         <dbl>            <int>       <int> <fct>\n1 Adelie  Torge‚Ä¶           39.5          17.4              186        3800 fema‚Ä¶\n2 Adelie  Torge‚Ä¶           40.3          18                195        3250 fema‚Ä¶\n3 Adelie  Torge‚Ä¶           36.7          19.3              193        3450 fema‚Ä¶\n4 Adelie  Torge‚Ä¶           38.9          17.8              181        3625 fema‚Ä¶\n5 Adelie  Torge‚Ä¶           41.1          17.6              182        3200 fema‚Ä¶\n6 Adelie  Torge‚Ä¶           36.6          17.8              185        3700 fema‚Ä¶\n# ‚Ä¶ with 1 more variable: year <int>\n\n\n\n\nAdditional operators\nYou can use %P.% to print the output of each step:\n\npenguins %P.% {\n  sex == \"female\"\n  select(1:5) \n  head(5)\n} -> y\n\npenguins %P.% {\n\n\n  sex == \"female\"\n\n\n# A tibble: 165 √ó 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.5          17.4               186        3800\n 2 Adelie  Torgersen           40.3          18                 195        3250\n 3 Adelie  Torgersen           36.7          19.3               193        3450\n 4 Adelie  Torgersen           38.9          17.8               181        3625\n 5 Adelie  Torgersen           41.1          17.6               182        3200\n 6 Adelie  Torgersen           36.6          17.8               185        3700\n 7 Adelie  Torgersen           38.7          19                 195        3450\n 8 Adelie  Torgersen           34.4          18.4               184        3325\n 9 Adelie  Biscoe              37.8          18.3               174        3400\n10 Adelie  Biscoe              35.9          19.2               189        3800\n# ‚Ä¶ with 155 more rows, and 2 more variables: sex <fct>, year <int>\n\n\n  select(1:5)\n\n\n# A tibble: 165 √ó 5\n   species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <fct>   <fct>              <dbl>         <dbl>             <int>\n 1 Adelie  Torgersen           39.5          17.4               186\n 2 Adelie  Torgersen           40.3          18                 195\n 3 Adelie  Torgersen           36.7          19.3               193\n 4 Adelie  Torgersen           38.9          17.8               181\n 5 Adelie  Torgersen           41.1          17.6               182\n 6 Adelie  Torgersen           36.6          17.8               185\n 7 Adelie  Torgersen           38.7          19                 195\n 8 Adelie  Torgersen           34.4          18.4               184\n 9 Adelie  Biscoe              37.8          18.3               174\n10 Adelie  Biscoe              35.9          19.2               189\n# ‚Ä¶ with 155 more rows\n\n\n  head(5)\n\n\n# A tibble: 5 √ó 5\n  species island    bill_length_mm bill_depth_mm flipper_length_mm\n  <fct>   <fct>              <dbl>         <dbl>             <int>\n1 Adelie  Torgersen           39.5          17.4               186\n2 Adelie  Torgersen           40.3          18                 195\n3 Adelie  Torgersen           36.7          19.3               193\n4 Adelie  Torgersen           38.9          17.8               181\n5 Adelie  Torgersen           41.1          17.6               182\n\n\n}\n\n\n%L.% prints out timings of each step in the sequence (Not particularly useful for this toy example, but would be for longer running code)\n\npenguins %L.% {\n  sex == \"female\"\n  select(1:5) \n  head(5)\n} -> z\n\npenguins %L.% {\n\n\n  sex == \"female\"\n\n\n   user  system elapsed \n  0.001   0.000   0.001 \n\n\n  select(1:5)\n\n\n   user  system elapsed \n  0.009   0.000   0.009 \n\n\n  head(5)\n\n\n   user  system elapsed \n  0.001   0.000   0.000 \n\n\n}\n\n\n%F.% lets you assign a function using the nakedpipe syntax:\n\npenguin_func <- . %F.% {\n  group_by(species)\n  summarise(across(where(is.numeric), ~mean(., na.rm = TRUE)))\n  mutate(across(where(is.numeric), round, 2))\n}\n\npenguin_func(penguins)\n\n# A tibble: 3 √ó 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  <fct>              <dbl>         <dbl>             <dbl>       <dbl> <dbl>\n1 Adelie              38.8          18.4              190.       3701. 2008.\n2 Chinstrap           48.8          18.4              196.       3733. 2008.\n3 Gentoo              47.5          15.0              217.       5076. 2008.\n\n\n\n\nggplot\nnakedpipe also plays well with ggplot:\n\npenguins %.% {\n  body_mass_kg = body_mass_g / 1000\n  ggplot(aes(body_mass_kg, flipper_length_mm, shape = species, colour = species))\n  + geom_point() \n  + labs(y = \"Flipper length\", x = \"Body mass (kg)\")\n  + theme_minimal()\n}\n\n\n\n\nThere are some other useful things that nakepipe can do that I haven‚Äôt included here, such as %D.% operator for debugging, %..% for even faster performance and even a RStudio addin to convert your magrittr style code to the nakepipe syntax. Pretty cool!\nIf you‚Äôve found any of these examples interesting, I‚Äôd highly recommend you check out the package (https://github.com/moodymudskipper/nakedpipe) and give it a try!\n\n\nReferences\n\n\n\n\n\nReferences\n\nBache, Stefan Milton, and Hadley Wickham. 2014. Magrittr: A Forward-Pipe Operator for r. https://CRAN.R-project.org/package=magrittr.\n\n\nFabri, Antoine. 2020. Nakedpipe: Pipe into a Sequence of Calls Without Repeating the Pipe Symbol. https://github.com/moodymudskipper/nakedpipe.\n\n\nKB, Gorman, Williams TD, and Fraser WR. 2014. ‚ÄúEcological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).‚Äù PLoS ONE 9(3) (e90081): ‚Äì13. https://doi.org/10.1371/journal.pone.0090081."
  },
  {
    "objectID": "posts/2021-10-31-oh-drat/index.html",
    "href": "posts/2021-10-31-oh-drat/index.html",
    "title": "Oh {drat}!",
    "section": "",
    "text": "Recently, I‚Äôve been investigating some approaches for creating your own CRAN-like repository for sharing a specific set of packages. There are a variety of solutions available for this problem (For example; RStudio package manager, RDepot and MiniCran), however I wanted to focus specifically on the great {drat} package by Dirk Eddelbuettel.\nCreating your own CRAN(ish) can sound a bit scary, at least it did to me before I started digging a bit deeper into {drat}, but fear not! There is great documentation and the steps are relatively easy to follow. I‚Äôve jotted down a few notes on how I did it, in the hope they might be of use for someone else trying to implement a similar thing.\n\n\nTo get started, you have two options:\n\nFork this repo, and follow these instructions.\nUse drat::initRepo(), specifying the basepath for your new repository (assuming you don‚Äôt want to use the default of ~/git).\n\nI opted for option 2, and the result was a simple repository in the right format. Also notice that initRepo creates a gh-pages, branch in your new directory. I left this as is, but you could also create the repository in the docs/ directory on the main branch by specifying the location argument.\n\n\nCheckout ?drat::initRepo() for more information.\nIn order to add a package to your repository, you first need to build the package (e.g., using R CMD build <package_name>). With the resulting tar.gz file you can run\n\ndrat::insertPackage(\"<package_name>.tar.gz\")\n\nand drat will take care of the rest.\nFinally, commit and push your changes to github to make your package available to download!\nFor example, first add your repository with the helper function drat::addRepo() and then install the package.\n\ndrat::addRepo(\"hfshr\")\ninstall.packages(\"qbr\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow getting this point felt pretty cool, but having to manually build the package each time seemed a bit too much like hard work‚Ä¶ I had an idea that using some kind of github action might be able to help, and after a bit of googling I found exactly what was needed. Fortunately, Michael Mahoney had written an excellent post on developing a github action to build a package and push it to the drat repository. I‚Äôll refer the reader to Michaels post, and this repo to see exactly how it works, but the code chunk below shows the workflow I added to my {qbr} package.\non:\n  push:\n    branches:\n      - 'main'\n  workflow_dispatch:\n\njobs:\n  drat-upload:\n    runs-on: ubuntu-20.04\n    env:\n      R_REMOTES_NO_ERRORS_FROM_WARNINGS: true\n      RSPM: \"https://packagemanager.rstudio.com/all/__linux__/focal/latest\"\n    name: Drat Upload\n    steps:\n      - uses: mikemahoney218/upload-to-drat-repo@v0.1\n        with:\n          drat_repo: 'hfshr/drat'\n          token: \"${{ secrets.DRAT_SECRET }}\"\n          commit_message: \"Automated update (add qbr)\"\n          commit_email: \"harryfisher21@gmail.com\"\nThe only parts you should need to change are the final four lines, where you;\n\nSpecify the name of your drat repository (e.g., <github username>/<drat repository name>)\nA personal access token saved as a github secret in your repository called DRAT_SECRET.\nA commit message that will appear in the drat repo.\nYour email for git commit.\n\n\n\nAgain, I would highly recommend reading Mike‚Äôs post for a more complete guide into setting this up.\nNow this work really well, however I also needed a way to update the information in my drat repository. Specifically, in my drat repo I have a README.Rmd that when knitted generates some information about the available packages. This README.Rmd also generates the index.html file that is used as the homepage for the repository, which you can see here and below:\n\n\n\n\n\n\n\n\n\nCurrently it is very basic, but does the job. I added a separate action to the drat repo inspired by this workflow that aims to do something very similar. My final result looks like this:\non:\n  push:\n    branches:\n      - 'gh-pages'\n  workflow_dispatch:\n\nname: drat-readme\n\njobs:\n  drat-readme:\n    name: Update README\n    runs-on: ubuntu-20.04\n    env:\n      R_REMOTES_NO_ERRORS_FROM_WARNINGS: true\n      RSPM: \"https://packagemanager.rstudio.com/all/__linux__/focal/latest\"\n\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          fetch-depth: '0' # Allows corect dates in git history\n\n      - uses: r-lib/actions/setup-r@master\n      - uses: r-lib/actions/setup-pandoc@master\n\n      - name: update repo indices # needed if tar balls removed manually\n        run: |\n          install.packages(\"drat\")\n          drat::updateRepo(\".\")\n        shell: Rscript {0}\n\n      - name: Drat -- update README.Rmd\n        if: success()\n        run: |\n          install.packages(c('rmarkdown', 'bslib', 'knitr'))\n          rmarkdown::render(\"README.Rmd\", output_format = \"html_document\", output_file = \"index.html\")\n          rmarkdown::render(\"README.Rmd\", output_format = \"github_document\")\n        shell: Rscript {0}\n\n      - name: Commit and push\n        if: success()\n        run: |\n          git config --global user.email \"actions@github.com\"\n          git config --global user.name  \"GitHub Actions\"\n          git commit README.md index.html -m \"Update README (auto)\" || echo \"Nothing to commit\"\n          git push origin || echo \"Nothing to commit\"\nAny pushes the the gh-pages branch of the drat repo will trigger this action to start. This action does a few things:\n\nChecks the drat repo is up to date by running drat::updateRepo().\nKnits the README.Rmd into both .md and .html formats, with the latter also being renamed to index.html so it serves as the homepage for the repo.\nCommits these changes with a short message.\n\nNow any time I update one of my packages that have the first github action implemented, the drat repo will also be updated! Pretty cool üòé (Well I thought so anyway..!) I‚Äôm sure this could be refined further, so any suggestions are more than welcome!"
  },
  {
    "objectID": "posts/2020-05-25-bitmexr-updates/index.html",
    "href": "posts/2020-05-25-bitmexr-updates/index.html",
    "title": "bitmexr 0.3.0: Place, modify and cancel your orders on BitMEX without leaving R!",
    "section": "",
    "text": "Install the latest release from CRAN:\nPackage site: https://hfshr.github.io/bitmexr/"
  },
  {
    "objectID": "posts/2020-05-25-bitmexr-updates/index.html#speed-boost",
    "href": "posts/2020-05-25-bitmexr-updates/index.html#speed-boost",
    "title": "bitmexr 0.3.0: Place, modify and cancel your orders on BitMEX without leaving R!",
    "section": "Speed boost",
    "text": "Speed boost\nWith authentication set up, map_trades() and map_bucket_trades() get a speed boost due to increased API limit when using authentication. Simply set use_auth = TRUE to get these functions running at 60 API call per minute."
  },
  {
    "objectID": "posts/2020-05-25-bitmexr-updates/index.html#additional-api-endpoints",
    "href": "posts/2020-05-25-bitmexr-updates/index.html#additional-api-endpoints",
    "title": "bitmexr 0.3.0: Place, modify and cancel your orders on BitMEX without leaving R!",
    "section": "Additional API endpoints",
    "text": "Additional API endpoints\nFor additional API endpoints that don‚Äôt have a dedicated API wrapper in bitmexr, it is possible use either:\n\nget_bitmex() for additional GET requests\n\nand\n\npost_bitmex() for additional POST requests.\n\nThese functions just need the path to the API endpoint and a named list of valid parameters for that endpoint. For example, you could use:\n\nchat <- get_bitmex(path = \"/chat\", \n                   args = list(channelID = 1, \n                              reverse = \"true\"))\n\nTo get the latest English trollbox chat data (not sure why anyone would want this, but it is possible!)."
  },
  {
    "objectID": "posts/2021-06-20-qbr/index.html",
    "href": "posts/2021-06-20-qbr/index.html",
    "title": "Introducing {qbr}.",
    "section": "",
    "text": "Feburary 2022 Update\nDear reader, if you‚Äôve just landed on this page, there have been some major changes the qbr since this post was written and you should checkout this post, or the documentation to get up to speed with the current version!\nI‚Äôve been spending quite a bit of time in the world of shiny recently, and a particular problem I was facing meant I decided to dip my toe into the seemingly magical world of htmlwidgets. In brief, I needed a way for users to interactively construct complex queries that would be used to interrogate a large database. While this is certainly possible to do using native shiny inputs, I was being lazy wanted to see if I could leverage something that had already been created. After a bit of googling I came across queryBuilder, a jquery plugin that provides UI component to create queries and filters. After playing around with the demos on the site, I was very impressed with the functionality and, and even better, someone had already made an R wrapper for the library! (See harveyl888/queryBuilder and Yannael/queryBuildR).\nHowever this was where I started to run into some problems. The original queryBuilder has a vast array of settings that can be configured, including additional plugins and widgets that help add additional functionality to the builder. The existing R wrappers for queryBuilder only implemented a few of these settings, and I found myself wanting to access other settings which were not yet implemented. So I thought I‚Äôd have a go at creating my own version, taking inspiration from harveyl888/queryBuilder with the aim of including as much of the functionality as possible of the original queryBuilder."
  },
  {
    "objectID": "posts/2021-06-20-qbr/index.html#example",
    "href": "posts/2021-06-20-qbr/index.html#example",
    "title": "Introducing {qbr}.",
    "section": "Example",
    "text": "Example\nYou can install qbr from github:\n\n#install.packages(\"remotes\")\nremotes::install_github(\"hfshr/qbr\")\n\nand also check out the repository here: hfshr/qbr\nHere is a quick example in shiny filtering the palmer penguins dataset with a few of the different settings configured. Code for the app is also below.\n\n\nhttps://harryfish.shinyapps.io/qbr_demo/\n\nlibrary(shiny)\nlibrary(qbr)\n\nui <- fluidPage(\n  # Application title\n  titlePanel(\"QueryBuilder demo\"),\n  mainPanel(\n    fluidRow(\n      column(\n        8,\n        queryBuilderOutput(\"querybuilder\",\n          width = 620,\n          height = 300\n        )\n      ),\n      column(\n        4,\n        uiOutput(\"txtValidation\")\n      )\n    )\n  ),\n  fluidRow(\n    column(\n      width = 6,\n      h3(\"dplyr filter\"),\n      verbatimTextOutput(\"txtFilterList\"),\n    ),\n    column(\n      width = 6,\n      h3(\"SQL filter\"),\n      verbatimTextOutput(\"txtSQL\")\n    )\n  ),\n  fluidRow(\n    column(\n      width = 12,\n      h3(\"dplyr filter applied to a table\"),\n      dataTableOutput(\"txtFilterResult\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver <- function(input, output) {\n  filters <- list(\n    list(\n      id = \"species\",\n      label = \"Species\",\n      type = \"string\",\n      input = \"select\",\n      description = \"Shift-click to select multiple!\",\n      values = list(\"Adelie\", \"Gentoo\", \"Chinstrap\"),\n      multiple = TRUE,\n      operators = c(\"equal\", \"not_equal\", \"in\")\n    ),\n    list(\n      id = \"sex\",\n      label = \"Sex\",\n      input = \"checkbox\",\n      values = list(\n        \"male\",\n        \"female\"\n      ),\n      operators = c(\"equal\", \"not_equal\", \"in\")\n    ),\n    list(\n      id = \"bill_length_mm\",\n      label = \"Bill length\",\n      type = \"integer\",\n      validation = list(\n        min = 0,\n        max = 100\n      ),\n      plugin = \"slider\",\n      plugin_config = list(\n        min = 0,\n        max = 100,\n        value = 0\n      )\n    )\n  )\n\n  output$txtValidation <- renderUI({\n    if (isFALSE(input$querybuilder_validate) || is.null(input$querybuilder_validate)) {\n      h3(\"INVALID QUERY\", style = \"color:red\")\n    } else {\n      h3(\"VALID QUERY\", style = \"color:green\")\n    }\n  })\n\n  output$querybuilder <- renderQueryBuilder({\n    queryBuilder(\n      filters = filters,\n      plugins = list(\n        \"sortable\" = NA,\n        \"bt-tooltip-errors\" = NA,\n        \"bt-checkbox\" = list(\"color\" = \"primary\"),\n        \"filter-description\" = list(\"mode\" = \"bootbox\"),\n        \"unique-filter\" = NA\n      ),\n      display_errors = TRUE,\n      allow_empty = FALSE,\n      select_placeholder = \"###\"\n    )\n  })\n\n  output$txtFilterList <- renderPrint({\n    req(input$querybuilder_validate)\n    filterTable(\n      filters = input$querybuilder_out,\n      data = palmerpenguins::penguins,\n      output = \"text\"\n    )\n  })\n\n\n  output$txtFilterResult <- renderDataTable(\n    {\n      req(input$querybuilder_validate)\n      filterTable(\n        filters = input$querybuilder_out,\n        data = palmerpenguins::penguins,\n        output = \"table\"\n      )\n    },\n    options = list(\n      pageLength = 5,\n      scrollY = \"200px\",\n      scrollX = TRUE\n    )\n  )\n\n  output$txtSQL <- renderPrint({\n    req(input$querybuilder_validate)\n    input$querybuilder_sql\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2021-06-20-qbr/index.html#summary",
    "href": "posts/2021-06-20-qbr/index.html#summary",
    "title": "Introducing {qbr}.",
    "section": "Summary",
    "text": "Summary\nThese settings barely scratch the surface of what is possible and the original queryBuilder site (https://querybuilder.js.org/) is well worth a visit to see the full potential of the widget. Also a special thanks to harveyl888/queryBuilder on which qbr is heavily based. If you find something that isn‚Äôt implemented feel free to open an issue hfshr/qbr.\nThanks for reading!"
  },
  {
    "objectID": "posts/2021-04-04-learning-rcpp/index.html",
    "href": "posts/2021-04-04-learning-rcpp/index.html",
    "title": "Zero to consciously incompetent with {Rcpp}.",
    "section": "",
    "text": "I was skimming through Hadley‚Äôs advanced R over Easter (as one does) and ended up spending quite a bit of time reading through the last chapter (anyone else read books back to front?), ‚ÄúRewriting R code in c++‚Äù. My understanding of c++ was (is?) pretty much non-existent, but whenever I try to learn something new I find the process of writing about it helps things stick. So this post is exactly that, me fumbling my way through some very basic c++ using {Rcpp}."
  },
  {
    "objectID": "posts/2021-04-04-learning-rcpp/index.html#acknowledgments",
    "href": "posts/2021-04-04-learning-rcpp/index.html#acknowledgments",
    "title": "Zero to consciously incompetent with {Rcpp}.",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nPreview image from https://github.com/RcppCore/Rcpp/issues/827, made by https://github.com/coatless"
  },
  {
    "objectID": "posts/2020-07-14-deploy-model/index.html",
    "href": "posts/2020-07-14-deploy-model/index.html",
    "title": "Build with R, deploy with Python (and Heroku).",
    "section": "",
    "text": "After going through many iterations of building, evaluating and refining a model, eventually the time may come to put that model into production. There are many options when it comes to deployment - in R, shiny and plumbr come to mind - However, in this post I wanted to see if it was possible to build a model in R, but deploy it using a python framework, specifically as a web app using flask. I also wanted to try and make the app available by using Heroku. Why you might ask? Good question! Not sure how useful this workflow is, but it was good fun getting it to work!\n(You can see the final app in action here: https://penguin-model.herokuapp.com/)"
  },
  {
    "objectID": "posts/2020-07-14-deploy-model/index.html#building-the-model",
    "href": "posts/2020-07-14-deploy-model/index.html#building-the-model",
    "title": "Build with R, deploy with Python (and Heroku).",
    "section": "Building the model",
    "text": "Building the model\nThe code below builds a very simple model that we‚Äôll use for deployment. This model attempts to predict the gender of a penguin based on bill length, bill depth and flipper length (data from the penguins package). You can imagine this model is going to be used on the local zoo‚Äôs website to make it more realistic (maybe)‚Ä¶\n\nlibrary(palmerpenguins)\nlibrary(xgboost)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(reticulate)\n\ndata(penguins)\n# feature engineering \npenguins <- penguins %>% \n  select(7, 3:5) %>% \n  drop_na() %>% \n  mutate(sex = ifelse(sex == \"female\", 1, 0))\n\n# training / testing split\nsplit <- initial_split(penguins, prop = .75)\ntrain <- training(split)\ntest <- testing(split)\n\n# convert to xgboost model matrix\ndtrain = xgb.DMatrix(data = as.matrix(train[,-1]), label = as.matrix(train$sex))\ndtest = xgb.DMatrix(data = as.matrix(test[,-1]), label = as.matrix(test$sex))\n\n# fit model\nxgb <- xgb.train(data = dtrain, \n                nrounds = 500, \n                params = list(objective = \"binary:logistic\",\n                              eval_metric = \"auc\"),\n                verbose = 0,\n                watchlist = list(eval = dtest))\n\n# predictions\npreds <- tibble(.pred_female = predict(xgb, as.matrix(test[,-1]))) %>% \n  bind_cols(test[,1], .) %>% \n  mutate(sex = factor(sex, levels = c(1,0), labels = c(\"female\", \"male\")))\n\n# evaluation\neval <- metric_set(roc_auc, gain_capture)\neval(preds, sex, .pred_female)\n\n# A tibble: 2 √ó 3\n  .metric      .estimator .estimate\n  <chr>        <chr>          <dbl>\n1 roc_auc      binary         0.922\n2 gain_capture binary         0.845\n\n\nNext we will save the xgboost model object using the xgb.save function from xgboost.\n\n# save model object\nxgboost::xgb.save(xgb, \"xgmod.model\")\n\nWith the model saved, we can now load it into python. Due to an issue with the python implementation of xgboost to load models from bytestring (see here: https://github.com/dmlc/xgboost/issues/3013), we have to use a workaround by defining the following function:\n\nimport ctypes\nimport xgboost\nimport xgboost.core\nimport pandas as pd\n\ndef xgb_load_model(buf):\n    if isinstance(buf, str):\n        buf = buf.encode()\n    bst = xgboost.core.Booster()\n    n = len(buf)\n    length = xgboost.core.c_bst_ulong(n)\n    ptr = (ctypes.c_char * n).from_buffer_copy(buf)\n    xgboost.core._check_call(\n        xgboost.core._LIB.XGBoosterLoadModelFromBuffer(bst.handle, ptr, length)\n    ) \n    return bst\n\nWe can now read in the xgboost model we created in R into python.\n\nwith open('xgmod.model','rb') as f:\n    raw = f.read()\n\nmodel = xgb_load_model(raw)\n\n# check to see if model loaded\nmodel\n\n<xgboost.core.Booster object at 0x7fd89f0b16d8>\n\n\nTo check the model is working and can generate predictions we can do:\n\n# create some mock data\ninput_variables = pd.DataFrame([[40, 20, 150]],\n                                columns=['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], \n                                dtype=float)\n                                \n                                \n# convert test datat to xgboost matrix\n\nxgtest = xgboost.DMatrix(input_variables.values)\n\n# predict\nprediction = model.predict(xgtest)[0]\n\nprint(prediction)\n\n0.003251487\n\n\nLooks good!"
  },
  {
    "objectID": "posts/2020-07-14-deploy-model/index.html#setting-up-deploment",
    "href": "posts/2020-07-14-deploy-model/index.html#setting-up-deploment",
    "title": "Build with R, deploy with Python (and Heroku).",
    "section": "Setting up deploment",
    "text": "Setting up deploment\nSo far we have created a model in R and loaded it to python and checked it works. So far so good. Next we can start to prepare for deployment.\nFor deployment, we will use flask to create a simple web app that lets users change the input values (bill length, bill width and flipper length) and returns the predicted probability of whether the penguin is female.\nFirst, we will create a directory called ‚Äúpenguin-app‚Äù with a few sub directories. You can create this anywhere, but I‚Äôve gone for my home directory desktop.\n\ndir_create(\"~/Desktop/penguin-app\")\ndir_create(\"~/Desktop/penguin-app/templates\")\ndir_create(\"~/Desktop/penguin-app/model\")\n\nNext, the python script below will define how the app works. This should be saved as app.py within the penguin-model directory.\n\nimport flask\nimport ctypes\nimport xgboost\nimport xgboost.core\nimport pandas as pd\n\n# function to load R model into python\ndef xgb_load_model(buf):\n    if isinstance(buf, str):\n        buf = buf.encode()\n    bst = xgboost.core.Booster()\n    n = len(buf)\n    length = xgboost.core.c_bst_ulong(n)\n    ptr = (ctypes.c_char * n).from_buffer_copy(buf)\n    xgboost.core._check_call(\n        xgboost.core._LIB.XGBoosterLoadModelFromBuffer(bst.handle, ptr, length)\n    )  # segfault\n    return bst\n\nwith open('model/xgmod.model','rb') as f:\n    raw = f.read()\n\nmodel = xgb_load_model(raw)\n\n# define the app\n\napp = flask.Flask(__name__, template_folder='templates')\n\n@app.route('/', methods=['GET', 'POST'])\n\ndef main():\n    if flask.request.method == 'GET':\n      return(flask.render_template('main.html'))\n\n    if flask.request.method == 'POST':\n      bill_length_mm = flask.request.form['bill_length_mm']\n      bill_depth_mm = flask.request.form['bill_depth_mm']\n      flipper_length_mm = flask.request.form['flipper_length_mm']\n      \n      input_variables = pd.DataFrame([[bill_length_mm, \n                                       bill_depth_mm, \n                                       flipper_length_mm]],\n                                      columns=['bill_length_mm', \n                                               'bill_depth_mm', \n                                               'flipper_length_mm'], dtype=float) \n      \n      xgtest = xgboost.DMatrix(input_variables.values)\n      \n      prediction = round(model.predict(xgtest)[0], 7)\n        \n      return flask.render_template('main.html', \n                                   original_input={'Bill length': bill_length_mm,\n                                                   'Bill depth': bill_depth_mm,\n                                               'Flipper length': flipper_length_mm},\n                                   result=prediction,)\n    \nif __name__ == '__main__':\n    app.run()\n\nThe code below is the template for our app. It includes some very simple css for style, javascript for some slider inputs to change the vales of the variables and a simple text box to output the results. This should be saved as main.html and put in the templates folder.\n\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css\">\n    <script src=\"https://code.jquery.com/jquery-3.2.1.slim.min.js\"></script>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js\"</script>\n    <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js\"</script>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  \n  <script type=\"text/javascript\">\n        function updateLengthInput(val) {\n                  document.getElementById('lengthInput').value=val; \n                }\n        function updateDepthInput(val) {\n                  document.getElementById('depthInput').value=val; \n                }\n        function updateFlipperInput(val) {\n                  document.getElementById('flipperInput').value=val; \n                }\n\n  </script>\n    \n  <style>\n        form {\n            margin: auto;\n            width: 40%;\n        }\n        \n        .result {\n            margin: auto;\n            width: 40%;\n            border: 1px solid #ccc;\n        }\n  </style>\n  \n  \n  <title>Penguin model</title>\n  \n</head>\n<body>\n  <form action=\"{{ url_for('main') }}\" method=\"POST\">\n    <fieldset>\n        <legend>Input values:</legend>\n        Bill length (mm):\n        <input name=\"bill_length_mm\" id=\"bill\" type=\"range\" min=\"30\" max=\"60\"\n        onchange=\"updateLengthInput(this.value);\" required>\n        <input type=\"text\" id=\"lengthInput\" value=\"45\">\n        <br>\n        <br> Bill depth (mm):\n        <input name=\"bill_depth_mm\" type=\"range\" min=\"10\" max=\"25\"\n        \n        onchange=\"updateDepthInput(this.value);\" required>\n        <input type=\"text\" id=\"depthInput\" value=\"17\">\n        <br>\n        <br> Flipper length (mm):\n        <input name=\"flipper_length_mm\" type=\"range\" min=\"170\" max=\"240\"\n        onchange=\"updateFlipperInput(this.value);\" required>\n        <input type=\"text\" id=\"flipperInput\" value=\"200\">\n        <br>\n        <br>\n        <div style=\"text-align:center\">  \n        <input type=\"submit\" />  \n        </div> \n    </fieldset>\n</form>\n<br>\n<br>\n<div class=\"result\" align=\"center\">\n    {% if result %}\n        {% for variable, value in original_input.items() %}\n            <b>{{ variable }}</b> : {{ value }}\n        {% endfor %}\n        <br>\n        <br> Probability of female penguin:\n           <p style=\"font-size:50px\">{{ result }}</p>\n    {% endif %}\n</div>  \n</body>\n</html>\n\nWe also need to move the saved model to the model directory. In the terminal we can use:\nmv xgmod.model penguin-app/model/\nAt this stage, it is useful to check if everything is working locally. To do this we can use flask run inside the penguin-app directory. This should make the app available at localhost:5000 which you can check in a browser."
  },
  {
    "objectID": "posts/2020-07-14-deploy-model/index.html#final-preparations",
    "href": "posts/2020-07-14-deploy-model/index.html#final-preparations",
    "title": "Build with R, deploy with Python (and Heroku).",
    "section": "Final preparations",
    "text": "Final preparations\nThere are a few minor steps left before we can complete deployment to Heroku and make this really useful model available to the world.\nFirst, the following is needed:\n\nA Heroku account,\nThe Heroku CLI tool\ngit\n\nWe also need to create the following files in the webapp directory:\nProcfile - This tells Heroku the type of app we are using and how to serve it to users.\n\nweb: gunicorn app:app\n\nRequirements.txt - This tells Heroku what packages need to be installed within your app\n\nflask\npandas\ngunicorn\nxgboost"
  },
  {
    "objectID": "posts/2020-07-14-deploy-model/index.html#deploy",
    "href": "posts/2020-07-14-deploy-model/index.html#deploy",
    "title": "Build with R, deploy with Python (and Heroku).",
    "section": "Deploy!",
    "text": "Deploy!\nWe are now ready to deploy the model. Over in the terminal, cd into the penguin-app directory (if not already there) and run:\n\ngit init: to initialise the git repo in the directory\nheroku login: this should open a web browser for you to log into your Heroku account\nheroku create penguin-model: To create our aptly named (pun intended) penguin-model app.\n\nNow we can add the contents of our app and push to heroku:\n\ngit add .\ngit commit -m \"First deployment\"\nheroku git:remote -a penguin-model\ngit push heroku master\n\nIf all goes to plan, that should result in a successful deployment! You can see the live app here: https://penguin-model.herokuapp.com/"
  },
  {
    "objectID": "posts/2020-07-14-deploy-model/index.html#summary",
    "href": "posts/2020-07-14-deploy-model/index.html#summary",
    "title": "Build with R, deploy with Python (and Heroku).",
    "section": "Summary",
    "text": "Summary\nSo that was a very quick tour of a cross-language model deployment. It was good fun getting each of these elements to work, and quite satisfying to see the final product. Furthermore, it was possible to get all of this woprking without ever leaving the comfort of RStudio - which is always a bonus! This post was heavily inspired this post, which provides a great introduction to the python/heroku side of deployment, and well worth a read if you‚Äôre interested.\nThanks for reading!"
  },
  {
    "objectID": "posts/2020-05-23-tidymodel-notes/index.html",
    "href": "posts/2020-05-23-tidymodel-notes/index.html",
    "title": "{tidymodels} workflow with Bayesian optimisation",
    "section": "",
    "text": "library(modeldata)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(doParallel)\nlibrary(probably)\nlibrary(gt)\n\ndata(\"credit_data\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    <fct> good, good, bad, good, good, good, good, good, good, bad, go‚Ä¶\n$ Seniority <int> 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, ‚Ä¶\n$ Home      <fct> rent, rent, owner, rent, rent, owner, owner, parents, owner,‚Ä¶\n$ Time      <int> 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, ‚Ä¶\n$ Age       <int> 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, ‚Ä¶\n$ Marital   <fct> married, widow, married, single, single, married, married, s‚Ä¶\n$ Records   <fct> no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes‚Ä¶\n$ Job       <fct> freelance, fixed, freelance, fixed, fixed, fixed, fixed, fix‚Ä¶\n$ Expenses  <int> 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, ‚Ä¶\n$ Income    <int> 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 19‚Ä¶\n$ Assets    <int> 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5‚Ä¶\n$ Debt      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000‚Ä¶\n$ Amount    <int> 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150,‚Ä¶\n$ Price     <int> 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 15‚Ä¶\n\n\nFirst, I‚Äôll split the sample into training and testing sets using rsample. As well as create v-fold cross-validation set for use in the tuning step later.\n\nset.seed(122)\n\ncredit_data <- credit_data %>%\n  drop_na(Status)\n\n# initial split\nsplit <- initial_split(credit_data, prop = 0.75, strata = \"Status\")\n\n# train/test sets\ntrain <- training(split)\ntest <- testing(split)\n\n# cross validation set\nfolds <- vfold_cv(train, v = 5)\n\n\n\nI‚Äôve dropped NA‚Äôs in outcome as these caused problems in later steps‚Ä¶\nNext, I‚Äôll set up the recipe using recipies. As well as defining the formula for the model, I‚Äôve created two preprocessing steps:\n\nImpute the missing values.\n\nOne-hot encode the factor variables in the data.\n\n\nrec <- recipe(Status ~ ., data = train) %>%\n  step_impute_bag(Home, Marital, Job, Income, Assets, Debt) %>%\n  step_dummy(Home, Marital, Records, Job, one_hot = T) \n\nNow I‚Äôll prepare the model specification using parsnip. Here I‚Äôm using an xgboost model and specify the parameters I want to tune using Bayesian optimisation. The mtry parameter requires one additional step to finalise the range of possible values (because it depends on the number of variables in the data and a suitable range of values to test can‚Äôt be estimated without that information).\n\nmod <- boost_tree(\n  trees = 1000,\n  min_n = tune(),\n  learn_rate = tune(),\n  loss_reduction = tune(),\n  sample_size = tune(),\n  mtry = tune(),\n  tree_depth = tune()\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"classification\")\n\n\nparams <- parameters(mod) %>%\n  finalize(train)\n\nIn this step I‚Äôve bundled all the above steps into a workflow. This avoids the need to use the juice, bake and prep functions (which I never quite got my head around‚Ä¶!).\n\nxgboost_wflow <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(mod)\n\nNow we are ready to OPTIMISE. I‚Äôm going to use an iterative search through Bayesian optimisation to predict what parameters to try next (as opposed to a grid search where we need to specific parameters values in advance).\nFirst I set up some addition works so the tests can be run in parallel, and then use the tune_bayes() function to set up the tuning. Here I‚Äôve decided to:\n\nLimit the number of iterations to 30\nUse precision-recall as the metric I want to optimise\nStart off with an initial grid of 10 combinations.\nStop early is no improvements are made after 10 iterations.\n\n\noptions(tidymodels.dark = TRUE)\n\ncl <- makePSOCKcluster(8)\nregisterDoParallel(cl)\n\ntuned <- tune_bayes(\n  object = xgboost_wflow,\n  resamples = folds,\n  param_info = params,\n  iter = 30,\n  metrics = metric_set(pr_auc),\n  initial = 10,\n  control = control_bayes(\n    verbose = TRUE,\n    no_improve = 10,\n    seed = 123\n  )\n)\n\n\n\nThe options(tidymodels.dark = TRUE) makes the text much clear if you‚Äôre using a dark theme in Rstudio.\nAfter a little while, we‚Äôre done! The top combinations were:\n\nshow_best(tuned, \"pr_auc\")%>% \n  select(1:7, 11) %>% \n  gt()\n\n\n\n\n  \n  \n    \n      mtry\n      min_n\n      tree_depth\n      learn_rate\n      loss_reduction\n      sample_size\n      .metric\n      std_err\n    \n  \n  \n    8\n13\n1\n0.080393032\n3.731578e+00\n0.4536060\npr_auc\n0.01687497\n    9\n17\n6\n0.018723128\n2.609860e-04\n0.5941547\npr_auc\n0.01547112\n    11\n19\n2\n0.063308769\n7.647379e-07\n0.6607560\npr_auc\n0.01278103\n    7\n8\n7\n0.002946107\n2.545899e-07\n0.5553494\npr_auc\n0.01094428\n    11\n18\n10\n0.053917413\n3.882588e-02\n0.4801454\npr_auc\n0.01518299\n  \n  \n  \n\n\n\n\n\n\nmean here is the average pr_auc across the resamples.\nNow we can create our final model using these parameters.\n\nxgboost_wkflow_tuned <- finalize_workflow(\n  xgboost_wflow,\n  select_best(tuned, \"pr_auc\")\n)\n\nFinally we can fit the model.\n\nfinal_res <- last_fit(\n  xgboost_wkflow_tuned,\n  split\n)\n\n#stopCluster(cl)\n\nWith our model in hand we can make some predictions to evaluate performance\n\npreds <- final_res %>% \n  collect_predictions()\n\nNow we can use the yardstick package to evaluate how the model performed.\n\nconf_mat(preds, Status, .pred_class)\n\n          Truth\nPrediction bad good\n      bad  167   76\n      good 147  724\n\npreds %>%\n  gain_curve(Status, .pred_bad) %>%\n  autoplot()\n\n\n\npreds %>%\n  pr_curve(Status, .pred_bad) %>%\n  autoplot()\n\n\n\n\n\n\nThe autoplot() feature is great for quickly visualising these curves\nhmmm‚Ä¶ That confusion matrix doesn‚Äôt look too great. Quite a large number ‚Äúgood‚Äù predictions were actually ‚Äúbad‚Äù (false negatives). Maybe when can improve the class prediction by using probably.\nHere we use threshold_perf() to evaluate different thresholds to make our class predictions. One methods to determine the ‚Äúbest‚Äù cut point is to use the j-index (maximum value of 1 when there are no false positives and no false negatives).\n\nthreshold_data <- preds %>%\n  threshold_perf(Status, .pred_bad, thresholds = seq(0.2, 1, by = 0.0025))\n\n\nmax_j_index_threshold <- threshold_data %>%\n  filter(.metric == \"j_index\") %>%\n  filter(.estimate == max(.estimate)) %>%\n  pull(.threshold)\n\n\npreds_new <- preds %>% \n  mutate(new_class_pred = factor(ifelse(.pred_bad >= max_j_index_threshold, \"bad\", \"good\"), \n                                 levels = c(\"bad\", \"good\"))) \n  \n\nmax_j_index_threshold\n\n[1] 0.2375\n\n\nNow we have a new prediction based on our new threshold of vs the default threshold of 0.50. We can compare the performance on a range of different binary classification metrics by calling summay() on the conf_mat() object for both the old and new predicted classes.\n\nsummary(conf_mat(preds, Status, .pred_class)) %>%\n  select(-.estimator) %>%\n  rename(old_threshold = .estimate) %>%\n  bind_cols(.,\n            summary(conf_mat(preds_new, Status, new_class_pred)) %>%\n              select(.estimate) %>%\n              rename(new_threshold = .estimate)) %>%\n  gt() %>%\n  fmt_number(columns = c(2, 3),\n             decimals = 3) %>%\n  tab_style(\n    style = cell_fill(color = \"indianred3\"),\n    locations = cells_body(columns = 3,\n                           rows = new_threshold < old_threshold)\n  )  %>%\n  tab_style(\n    style = cell_fill(color = \"springgreen3\"),\n    locations = cells_body(columns = 3,\n                           rows = new_threshold > old_threshold)\n  ) %>% \n  cols_align(align = \"center\")\n\n\n\n\n  \n  \n    \n      .metric\n      old_threshold\n      new_threshold\n    \n  \n  \n    accuracy\n0.800\n0.736\n    kap\n0.469\n0.447\n    sens\n0.532\n0.825\n    spec\n0.905\n0.701\n    ppv\n0.687\n0.520\n    npv\n0.831\n0.911\n    mcc\n0.476\n0.476\n    j_index\n0.437\n0.526\n    bal_accuracy\n0.718\n0.763\n    detection_prevalence\n0.218\n0.447\n    precision\n0.687\n0.520\n    recall\n0.532\n0.825\n    f_meas\n0.600\n0.638\n  \n  \n  \n\n\n\n\nLowering the threshold to .27 seems to have had a positive impact on quite a few of the binary classification metrics. There is always going to be a trade off between maximising some metrics over others, and will of course depend on what you are trying to achieve with your model.\n‚Ä¶and that is a very quick tour of tidymodels! There are obviously some additional steps you would want to carry out out in the ‚Äúreal‚Äù world. You‚Äôd probably want to compare a range of different models and maybe do some additional feature engineering based on the data you have, but the code above is a good initial starting point for a tidymodels orientated workflow.\n\n\nThere are many other features in tidymodels and I‚Äôve barely scratched the surface here!\nThanks for reading!"
  },
  {
    "objectID": "posts/2021-01-17-rstudio-certification/index.html",
    "href": "posts/2021-01-17-rstudio-certification/index.html",
    "title": "Musings on the RStudio instructor training process",
    "section": "",
    "text": "Recently, I became a certified RStudio instructor and thought I‚Äôd share a few of my thoughts on the process.\nFirst, a little background‚Ä¶\nDuring my PhD, I discovered R almost by accident while I was battling the twin-headed demon of Microsoft excel and SPSS, trying with all my might to reshape my data and run some kind (any kind‚Ä¶!) of analysis.\nAt some point I came across reshape2::melt which described exactly what I wanted to do with my data at that point in time, melt it into oblivion‚Ä¶ However, with a bit more googling I actually figured out how to get my data from wide to long format. The sense of accomplishment was palpable. I was hooked.\nBy the end of my PhD I had written my thesis completely in R markdown, worked closely with the author of bnlearn who helped shaped the analysis I undertook, and managed to get my first job as a data scientist in the financial crime and compliance sector, an alien world for someone with a sports science PhD, but learning R had given me a viable set of skills that someone would actually pay me to use‚Ä¶ Who‚Äôd of thought!\nWhile my journey into R was a little haphazard and certainly took me far longer to figure things out than it should have, I really wanted to try and share my experiences and help someone else who may be in a similar position to me when I first started."
  },
  {
    "objectID": "posts/2021-01-17-rstudio-certification/index.html#section",
    "href": "posts/2021-01-17-rstudio-certification/index.html#section",
    "title": "Musings on the RStudio instructor training process",
    "section": "",
    "text": "Here is my badge of honour üòÉ\n\nknitr::include_graphics(\"certificate.png\")"
  },
  {
    "objectID": "posts/2021-01-29-many-models-many-plots/index.html",
    "href": "posts/2021-01-29-many-models-many-plots/index.html",
    "title": "Many models, many plots, many pages!",
    "section": "",
    "text": "I wanted to share a cool knitr trick I recently discovered thanks to this excellent SO post."
  },
  {
    "objectID": "posts/2021-01-29-many-models-many-plots/index.html#the-problem",
    "href": "posts/2021-01-29-many-models-many-plots/index.html#the-problem",
    "title": "Many models, many plots, many pages!",
    "section": "The problem",
    "text": "The problem\n\nYou have some data that has many groups.\nYou want to fit a model and create a plot of the result for each group.\nYou want to create a pdf output that has each plot for each group on a new page, with the correct figure title and list of figures.\nYou don‚Äôt want to write each code chunk out manually."
  },
  {
    "objectID": "posts/2021-01-29-many-models-many-plots/index.html#the-solution",
    "href": "posts/2021-01-29-many-models-many-plots/index.html#the-solution",
    "title": "Many models, many plots, many pages!",
    "section": "The solution",
    "text": "The solution\nFor a quick demonstration, lets use the survival::verteran dataset. We want to fit a survival model for each celltype and use ggsurvplot to create a survival curve for each model. Finally we want to print out each of the plots on a new page in a pdf document.\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úì ggplot2 3.3.5     ‚úì purrr   0.3.4\n‚úì tibble  3.1.6     ‚úì dplyr   1.0.8\n‚úì tidyr   1.2.0     ‚úì stringr 1.4.0\n‚úì readr   2.1.1     ‚úì forcats 0.5.1\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(survival)\nlibrary(survminer)\n\nLoading required package: ggpubr\n\n\n\nAttaching package: 'survminer'\n\n\nThe following object is masked from 'package:survival':\n\n    myeloma\n\n# fit model for each celltype\n\nby_celltype <- veteran %>% \n  nest_by(celltype) %>% \n  mutate(model = list(survfit(Surv(time, status) ~ trt, data = data)),\n         plot = list(ggsurvplot(model, data = data)),\n         name = str_glue(\"Survival curve by treatmeant for celltype: {celltype}\"))\n\nIf you haven‚Äôt seen nest_by in action before, I‚Äôd highly recommend checking out this dplyr article.\nNext we extract the list-column by_celltype$plot as a list, and give each element the name that was created using str_glue in the previous step.\n\nall_plots <- as.list(by_celltype$plot)\n\nnames(all_plots) <- by_celltype$name\n\nnames(all_plots)\n\n[1] \"Survival curve by treatmeant for celltype: squamous\" \n[2] \"Survival curve by treatmeant for celltype: smallcell\"\n[3] \"Survival curve by treatmeant for celltype: adeno\"    \n[4] \"Survival curve by treatmeant for celltype: large\"    \n\n\nThese will be used as our figure captions in the pdf.\nIn this final step, we can pass in the names of the list to the fig.cap option in the knitr chunk, and set results = 'asis' like so\n```{r fig.cap=names(all_plots), results='asis'}\n```\n\nIn that chunk we can use a loop that prints each element in our list (in this case our plots) and also a \\newpage command after each plot.\n\n\nNote that we need to use two \\\\ in order escape the single \\ in markdown.\nfor(plot in names(all_plots)){\n  print(all_plots[[plot]])\n  cat('\\\\newpage')\n}\nBelow is an example of the output.\n\n\nNow, this was only with three groups, so it probably wouldn‚Äôt have been too much trouble to write out manually. However with 50 or 100 groups, this workflow can come in very handy!"
  },
  {
    "objectID": "posts/2021-01-29-many-models-many-plots/index.html#acknowledgments",
    "href": "posts/2021-01-29-many-models-many-plots/index.html#acknowledgments",
    "title": "Many models, many plots, many pages!",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nAll thanks goes to Michael Harper for his great answer to this stack overflow post: https://stackoverflow.com/questions/52788015/looping-through-a-list-of-ggplots-and-giving-each-a-figure-caption-using-knitr"
  },
  {
    "objectID": "posts/2021-05-03-dm-is-fantastic/index.html",
    "href": "posts/2021-05-03-dm-is-fantastic/index.html",
    "title": "{dm} is fantastic!",
    "section": "",
    "text": "Sometimes as an R user you may find yourself needing to work with data that is stored in a remote database, but want to continue to use your R-orientated workflow. The dm package by Kirill M√ºller provides some great tools for doing exactly that, and in this post I wanted to share some of the functions I‚Äôve found particularly useful.\nNow, you may already be familiar with the dbplyr package which allows you to use the majority of dplyr/tidyr functions on a remote table by auto-magically converting these commands into the necessary SQL and returning the result back to R when you call collect(). dm extends this concept and adds extra functionality for working with the whole database, making it a very valuable tool. For the examples in this post, I‚Äôve set up a local SQL server, and copied over the dm::dm_nycflights13() example database that is included in the dm package.\nNext I rerun the connection, but this time specifying the newly created nycflights database.\nNow our data is in the database we can begin working with it. To connect to the database and learn the connections, we can use dm_from_src(). Setting the learn_keys argument to TRUE means dm will attempt to discover the primary and foreign keys between the tables, however this currently only works with Postgres and SQL Server databases.\nA really great feature of dm is the ability to plot the database to visualise the links between tables. We can do this by using the dm_draw() function\nNow, there is a small issue with the nycflights data in that some of the tailnum values are not present in both the flights and planes table. However, we can manually create the link by adding an additional foreign key using dm_add_fk().\nAnd if we draw the model again we can see that planes is now connected to the flights table. I‚Äôve also added some extra styling to show how easy it is to customise these plots with dm."
  },
  {
    "objectID": "posts/2021-05-03-dm-is-fantastic/index.html#working-with-the-data",
    "href": "posts/2021-05-03-dm-is-fantastic/index.html#working-with-the-data",
    "title": "{dm} is fantastic!",
    "section": "Working with the data",
    "text": "Working with the data\nSo, now we have a data model set up we can begin working with it. dm contains a range of dm_* functions that work in a similar way to their dpylr equivalents, but they can affect the whole data model object. For example, we can select certain tables and use filters on the whole data model. Lets drop the weather table as we aren‚Äôt too interested in weather for the time being.\n\nflights <- flights %>% \n  dm_select_tbl(-weather)\n\nflights\n#> ‚îÄ‚îÄ Table source ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> src:  Microsoft SQL Server 15.00.4138[dbo@h-xps/nycflights]\n#> ‚îÄ‚îÄ Metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Tables: `airlines`, `airports`, `flights`, `planes`, `mtcars`\n#> Columns: 49\n#> Primary keys: 3\n#> Foreign keys: 3\n\n\nBy printing the dm object, we can see the weather table is no longer part of the dm object.\nLets say we are particular interested in finding out about Alaska Airlines Inc.¬†We can use dm_filter() to find all the linked information for this airline in our data model.\n\nflights_filt <- flights %>% \n  dm_filter(flights, carrier == \"AA\") \n\nflights_filt\n#> ‚îÄ‚îÄ Table source ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> src:  Microsoft SQL Server 15.00.4138[dbo@h-xps/nycflights]\n#> ‚îÄ‚îÄ Metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Tables: `airlines`, `airports`, `flights`, `planes`, `mtcars`\n#> Columns: 49\n#> Primary keys: 3\n#> Foreign keys: 3\n#> ‚îÄ‚îÄ Filters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> flights: carrier == \"AA\"\n\n\nNow we can see we have some additional information regarding the filter when we print the data model. In order to apply this filter, we have a couple of possibilities. If we were interested in returning all the rows in the airports table, we could apply this filter directly to that table using dm_apply_filters_to_tbl().\n\nflights_filt %>% \n  dm_apply_filters_to_tbl(airports)\n#> # Source:   lazy query [?? x 8]\n#> # Database: Microsoft SQL Server 15.00.4138[dbo@h-xps/nycflights]\n#>   faa   name                  lat   lon   alt    tz dst   tzone           \n#>   <chr> <chr>               <dbl> <dbl> <dbl> <dbl> <chr> <chr>           \n#> 1 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York\n#> 2 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n#> 3 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n\n\nSometimes, we might want to return all tables with the filter applied. dm provides the dm_flattern_to_tbl() function to do exactly that. First we need to apply the filter to the dm object, and then we can ‚Äúflattern‚Äù to a single table, specifying the type of join we would like to use. dm will create the joins based on the keys already defined in the dm object.\n\nflights_filt %>% \n  dm_apply_filters() %>% \n  dm_flatten_to_tbl(start = flights, airports, planes, join = left_join)\n\n#> Renamed columns:\n#> * year -> flights.year, planes.year\n\n#> # Source:   lazy query [?? x 34]\n#> # Database: Microsoft SQL Server 15.00.4138[dbo@h-xps/nycflights]\n#>    flights.year month   day dep_time sched_dep_time dep_delay arr_time\n#>           <int> <int> <int>    <int>          <int>     <dbl>    <int>\n#>  1         2013     1    10      531            540        -9      832\n#>  2         2013     1    10      553            600        -7      837\n#>  3         2013     1    10      555            600        -5      733\n#>  4         2013     1    10      604            610        -6      851\n#>  5         2013     1    10      604            610        -6      858\n#>  6         2013     1    10      625            630        -5      753\n#>  7         2013     1    10      633            630         3     1142\n#>  8         2013     1    10      652            659        -7      942\n#>  9         2013     1    10      659            700        -1     1013\n#> 10         2013     1    10      700            700         0      837\n#> # ‚Ä¶ with more rows, and 27 more variables: sched_arr_time <int>,\n#> #   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, name <chr>, lat <dbl>, lon <dbl>,\n#> #   alt <dbl>, tz <dbl>, dst <chr>, tzone <chr>, planes.year <int>,\n#> #   type <chr>, manufacturer <chr>, model <chr>, engines <int>,\n#> #   seats <int>, speed <int>, engine <chr>\n\n\nThe start argument is the table that we want to join the others onto. You can optionally supply tables names that you want to join (in this case airports and planes), or specify nothing to join all tables. Finally you can also specify the type of join you would like to use (left_join is the default).\nBefore flattening to a single table, it can sometimes be useful to see how many rows each tables has after you have applied the filter.\n\nflights_filt %>% \n  dm_apply_filters() %>% \n  dm_nrow()\n\n#> airlines airports  flights   planes   mtcars \n#>        1        3     1089      118       32\n\n\nThis can be good to double check the filter has done what you think it was going to do!\nTo see what‚Äôs going on under the hood here, we can use show_query().\n\nflights_filt %>% \n  dm_apply_filters() %>% \n  dm_flatten_to_tbl(start = flights, airports, planes, join = left_join) %>% \n  show_query()\n\n#> Renamed columns:\n#> * year -> flights.year, planes.year\n\n\n#> <SQL>\n#> SELECT \"flights.year\", \"month\", \"day\", \"dep_time\", \"sched_dep_time\", \"dep_delay\", \"arr_time\", \"sched_arr_time\", \"arr_delay\", \"carrier\", \"flight\", \"LHS\".\"tailnum\" AS \"tailnum\", \"origin\", \"dest\", \"air_time\", \"distance\", \"hour\", \"minute\", \"time_hour\", \"name\", \"lat\", \"lon\", \"alt\", \"tz\", \"dst\", \"tzone\", \"planes.year\", \"type\", \"manufacturer\", \"model\", \"engines\", \"seats\", \"speed\", \"engine\"\n#> FROM (SELECT \"flights.year\", \"month\", \"day\", \"dep_time\", \"sched_dep_time\", \"dep_delay\", \"arr_time\", \"sched_arr_time\", \"arr_delay\", \"carrier\", \"flight\", \"tailnum\", \"origin\", \"dest\", \"air_time\", \"distance\", \"hour\", \"minute\", \"time_hour\", \"name\", \"lat\", \"lon\", \"alt\", \"tz\", \"dst\", \"tzone\"\n#> FROM (SELECT \"year\" AS \"flights.year\", \"month\", \"day\", \"dep_time\", \"sched_dep_time\", \"dep_delay\", \"arr_time\", \"sched_arr_time\", \"arr_delay\", \"carrier\", \"flight\", \"tailnum\", \"origin\", \"dest\", \"air_time\", \"distance\", \"hour\", \"minute\", \"time_hour\"\n#> FROM \"dbo\".\"flights\"\n#> WHERE (\"carrier\" = 'AA')) \"LHS\"\n#> LEFT JOIN (SELECT * FROM \"dbo\".\"airports\" AS \"LHS\"\n#> WHERE EXISTS (\n#>   SELECT 1 FROM (SELECT *\n#> FROM \"dbo\".\"flights\"\n#> WHERE (\"carrier\" = 'AA')) \"RHS\"\n#>   WHERE (\"LHS\".\"faa\" = \"RHS\".\"origin\")\n#> )) \"RHS\"\n#> ON (\"LHS\".\"origin\" = \"RHS\".\"faa\")\n#> ) \"LHS\"\n#> LEFT JOIN (SELECT \"tailnum\", \"year\" AS \"planes.year\", \"type\", \"manufacturer\", \"model\", \"engines\", \"seats\", \"speed\", \"engine\"\n#> FROM (SELECT * FROM \"dbo\".\"planes\" AS \"LHS\"\n#> WHERE EXISTS (\n#>   SELECT 1 FROM (SELECT *\n#> FROM \"dbo\".\"flights\"\n#> WHERE (\"carrier\" = 'AA')) \"RHS\"\n#>   WHERE (\"LHS\".\"tailnum\" = \"RHS\".\"tailnum\")\n#> )) \"q01\") \"RHS\"\n#> ON (\"LHS\".\"tailnum\" = \"RHS\".\"tailnum\")\n\n\nSo with just a few line of code we were able to generate this fairly lengthy SQL statement. Pretty neat üòé."
  },
  {
    "objectID": "posts/2021-05-03-dm-is-fantastic/index.html#zooming-with-dms",
    "href": "posts/2021-05-03-dm-is-fantastic/index.html#zooming-with-dms",
    "title": "{dm} is fantastic!",
    "section": "Zooming with dm‚Äôs",
    "text": "Zooming with dm‚Äôs\nAnther cool feature in dm is the ability to ‚Äúzoom‚Äù into a table and use a dplyr style workflow on that table. This will be very familiar if you have ever used dbplyr. Lets zoom into the planes table and find the mean number of seats for each manufacturer.\n\nzoomed <- flights_filt %>% \n  dm_zoom_to(planes) %>% \n  group_by(manufacturer) %>% \n  summarise(mean_seats = mean(seats, na.rm = TRUE)) %>% \n  arrange(desc(mean_seats))\n\nzoomed\n#> # Zoomed table: planes\n#> # Source:       lazy query [?? x 2]\n#> # Database:     Microsoft SQL Server 15.00.4138[dbo@h-xps/nycflights]\n#> # Ordered by:   desc(mean_seats)\n#>    manufacturer                  mean_seats\n#>    <chr>                              <int>\n#>  1 AIRBUS                               221\n#>  2 AIRBUS INDUSTRIE                     187\n#>  3 BOEING                               175\n#>  4 MCDONNELL DOUGLAS                    162\n#>  5 MCDONNELL DOUGLAS CORPORATION        142\n#>  6 MCDONNELL DOUGLAS AIRCRAFT CO        142\n#>  7 DOUGLAS                              102\n#>  8 BOMBARDIER INC                        74\n#>  9 CANADAIR                              55\n#> 10 EMBRAER                               45\n#> # ‚Ä¶ with more rows\n\n\nAfter using dm_zoom_to() to select the planes table the rest of the code should look fairly familiar. We now have a couple of options for what we want to do with our new zoomed in table. We might want to overwrite the existing planes table with our new summary table. If that was the case we could use the dm_update_zoomed() to replace the original table with the one we have just created. An alternative (a potentially less destructive) approach is to create a new table containing the summary information.\n\nflights_updated <- zoomed %>% \n  dm_insert_zoomed(new_tbl_name = \"planes_summary\")\n\nflights_updated\n#> ‚îÄ‚îÄ Table source ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> src:  Microsoft SQL Server 15.00.4138[dbo@h-xps/nycflights]\n#> ‚îÄ‚îÄ Metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> Tables: `airlines`, `airports`, `flights`, `planes`, `mtcars`, `planes_summary`\n#> Columns: 51\n#> Primary keys: 3\n#> Foreign keys: 3\n#> ‚îÄ‚îÄ Filters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#> flights: carrier == \"AA\"\n\n\nNow this particular summary table doesn‚Äôt need to link to any of the other tables, but the dm zoomed vingette provides lots of great examples for how one might link summary tables into the dm object.\n\ndm_draw(flights_updated)\n\n\n\n\n\nHowever, we may want to access this table and include it in our report. We can access a specific table easily by using the pull_tbl() function.\n\nflights_updated %>% \n  pull_tbl(planes_summary) %>% \n  collect() %>% \n  reactable::reactable()\n\n\n\n\n\nNice!"
  },
  {
    "objectID": "posts/2021-05-03-dm-is-fantastic/index.html#conclusion",
    "href": "posts/2021-05-03-dm-is-fantastic/index.html#conclusion",
    "title": "{dm} is fantastic!",
    "section": "Conclusion",
    "text": "Conclusion\nSo that was a quick overview of some of the functions I‚Äôve found particularly useful. dm can do much more though so check out the site https://cynkra.github.io/dm/index.html which contains a great mix of tutorials and technical articles which make the package extremely accessible and fun to use.\nThanks for reading!"
  },
  {
    "objectID": "posts/2020-03-08-athletics-rankings/index.html",
    "href": "posts/2020-03-08-athletics-rankings/index.html",
    "title": "Athletics rankings",
    "section": "",
    "text": "As a keen athlete I spent many hours on athletics rankings website https://www.thepowerof10.info. For athletics nerds, the site is fantastic and provides all the information you need to keep up to date with the latest results. However, I wondered if it might be possible to explore some alternative ways of presenting the results‚Ä¶"
  },
  {
    "objectID": "posts/2020-03-08-athletics-rankings/index.html#individual-athletes",
    "href": "posts/2020-03-08-athletics-rankings/index.html#individual-athletes",
    "title": "Athletics rankings",
    "section": "Individual athletes",
    "text": "Individual athletes\nThe PO10 also provides detailed performance history for individual athletes. I wanted to be able to have access to this data as well, however I wanted to avoid downloading every individual athletes data to disk as I imagine that may have taken a while‚Ä¶\nInstead the solution I came up with was a function get each individual athlete‚Äôs unique identifier number. With this number I could get each athletes individual rankings when required using an ‚Äúon the fly‚Äù scrape, as a single athletes page is not very much data at all.\nThe screenshot below shows how the code that I needed to extract\n\nknitr::include_graphics(\"images/po10id.png\")\n\n\n\n\nFigure from https://www.thepowerof10.info\n\n\n\n\nThe function below collects the athltes name, unique url, year and event.\n\nathleteurl <- function(url) {\n  main <- read_html(url)\n  athleteinfo <- tibble(\n    name = html_text(html_nodes(main, \"td:nth-child(7) a\"), \"href\"),\n    athleteurl = html_attr(html_nodes(main, \"td:nth-child(7) a\"), \"href\"),\n    year = as.numeric(str_extract_all(url, \"[0-9]+\")[[1]][[3]]),\n    event = as.numeric(str_extract_all(url, \"[0-9]+\")[[1]][[2]])\n  ) %>%\n    filter(name != \"\")\n}\n\nids <- athleteurl(url)\n\nhead(ids)\n\n# A tibble: 6 √ó 4\n  name                   athleteurl                               year event\n  <chr>                  <chr>                                   <dbl> <dbl>\n1 Dwain Chambers         /athletes/profile.aspx?athleteid=31816   2012   100\n2 Adam Gemili            /athletes/profile.aspx?athleteid=208735  2012   100\n3 James Dasaolu          /athletes/profile.aspx?athleteid=22721   2012   100\n4 Harry Aikines-Aryeetey /athletes/profile.aspx?athleteid=19988   2012   100\n5 Mark Lewis-Francis     /athletes/profile.aspx?athleteid=21139   2012   100\n6 James Alaka            /athletes/profile.aspx?athleteid=22255   2012   100\n\n\nIn another function I join these two tables together so I had one data frame that had all the results and rankings as well as each athletes individual id that I could use to get their individual data. I also appended the full address to each individual athletes id.\n\nfinaljoin <- function(ranks, ids, gender) {\n  yeartimes <- ranks %>%\n    group_by(year, event) %>%\n    filter(str_detect(rank, \"[:alpha:]\")) %>%\n    select(year, rank) %>%\n    filter(str_detect(rank, \"^UK\")) %>%\n    separate(rank, c(\"topn\", \"timing\"), \": \") %>%\n    spread(key = topn, value = timing) %>%\n    ungroup()\n  cleanrankings <- ranks %>%\n    filter(!str_detect(rank, \"[:alpha:]\")) %>%\n    inner_join(., ids, by = c(\"name\", \"year\", \"event\")) %>%\n    mutate(athleteurl = paste(\"https://www.thepowerof10.info\", athleteurl, sep = \"\"))\n\n  cleanrankings <- left_join(cleanrankings, yeartimes, by = c(\"event\", \"year\")) %>%\n    janitor::clean_names() %>%\n    mutate(gender = gender)\n}\n\nThis left me with a complete dataframe I could work with, with the options of getting individual athletes data as needed,\n\nclean_ranks <- finaljoin(ranks = ranks, ids = ids, gender = \"M\")\n\nAdding missing grouping variables: `event`\n\nclean_ranks %>%\n  head() %>%\n  select(rank, perf, name, year, club, venue, athleteurl) %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nrank\nperf\nname\nyear\nclub\nvenue\nathleteurl\n\n\n\n\n1\n10.02\nDwain Chambers\n2012\nBelgrave\nOlympic Park\nhttps://www.thepowerof10.info/athletes/profile.aspx?athleteid=31816\n\n\n2\n10.05\nAdam Gemili\n2012\nBlackheath & Bromley\nBarcelona, ESP\nhttps://www.thepowerof10.info/athletes/profile.aspx?athleteid=208735\n\n\n3\n10.13\nJames Dasaolu\n2012\nCroydon\nOlympic Park\nhttps://www.thepowerof10.info/athletes/profile.aspx?athleteid=22721\n\n\n4\n10.20\nHarry Aikines-Aryeetey\n2012\nSutton & District\nRovereto, ITA\nhttps://www.thepowerof10.info/athletes/profile.aspx?athleteid=19988\n\n\n5\n10.21\nMark Lewis-Francis\n2012\nBirchfield H\nMesa AZ, USA\nhttps://www.thepowerof10.info/athletes/profile.aspx?athleteid=21139\n\n\n6\n10.22\nJames Alaka\n2012\nBlackheath & Bromley\nEugene OR, USA\nhttps://www.thepowerof10.info/athletes/profile.aspx?athleteid=22255\n\n\n\n\n\nThe final function I used was to scrape and clean each individuals rankings ‚Äúon the fly‚Äù. The only input is an athletes individual url. The result was another dataframe that contains an individual athletes history of performances.\n\nindividual <- function(athlete) {\n  history <- athlete %>%\n    read_html() %>%\n    html_nodes(xpath = '//*[@id = \"cphBody_pnlPerformances\"]/table') %>%\n    html_table(fill = TRUE) %>%\n    .[[2]] %>%\n    select(-c(X4, X5, X8, X9)) %>%\n    set_names(c(\"event\", \n                \"perf\", \n                \"indoor\", \n                \"position\", \n                \"heat\", \n                \"venue\", \n                \"meeting\", \n                \"date\")) %>%\n    filter(!str_detect(event, \"[:alpha:]\")) %>%\n    mutate(\n      date = dmy(date),\n      year = substr(date, 1, 4),\n      perf_time = case_when(\n        str_detect(perf, \":\") == FALSE ~ str_c(\"00:00:\", perf),\n        str_length(str_split_fixed(perf, \".\", 4)[4]) == 3 ~ str_c(perf, \"0\"),\n        str_length(perf) == 6 | str_length(perf) == 7 ~ str_c(\"00:0\", perf),\n        str_length(perf) == 8 | str_length(perf) == 9 ~ str_c(\"00:\", perf),\n        TRUE ~ perf\n      )\n    )\n  name <- read_html(athlete)\n  name <- html_text(html_nodes(name, css = \"h2\"), trim = TRUE)\n  history <- history %>%\n    mutate(name = name)\n  return(history)\n}\n\n\nathlete <- \"https://www.thepowerof10.info/athletes/profile.aspx?athleteid=31816\"\n\nindividual(athlete) %>%\n  head(5) %>%\n  select(name, event, perf, position, venue, meeting, date) %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nevent\nperf\nposition\nvenue\nmeeting\ndate\n\n\n\n\nDwain Chambers\n100\n10.91\n4\nSportcity\nM√ºller British Championships inc. Invitation 10000m\n2021-06-25\n\n\nDwain Chambers\n60\n6.73\n1\nLee Valley\nSouth of England AA U20 / Senior Championships\n2020-02-01\n\n\nDwain Chambers\n60\n6.74\n1\nLee Valley\nSouth of England AA U20 / Senior Championships\n2020-02-01\n\n\nDwain Chambers\n60\n6.76\n2\nLee Valley\nLondon U20 / Senior Games\n2020-01-19\n\n\nDwain Chambers\n60\n6.77\n5\nLee Valley\nScienhealth Athletics Invitational\n2020-01-05\n\n\n\n\n\nSo that‚Äôs it! I am currently in the process of creating an interactive dashboard to visualise these results. You can see an early version here:\nhttps://harryfish.shinyapps.io/resultsdashboard/\nLots of work left to do, but any comments or feedback are always welcome. The source code is on my github if you want to try anything out.\nThanks for reading!"
  },
  {
    "objectID": "posts/2020-03-14-writing-a-thesis-in-r-markdown/index.html",
    "href": "posts/2020-03-14-writing-a-thesis-in-r-markdown/index.html",
    "title": "Writing a thesis in R Markdown",
    "section": "",
    "text": "The purpose of this post is to pay homage to the resources I used throughout writing my PhD and highlight some of the issues I came across while doing so, in the hope that a future reader may find something useful‚Ä¶\nMy only advice to someone who was considering using R Markdown to write a thesis would be just go for it - there are ample resources out there to help, and the process is incredibly rewarding.\n\n\nFor those interested, the repository of my completed thesis can be found here: https://github.com/hfshr/phd-thesis\n\nBenefits\n\nCombine text and code\nThe ability to integrate text and R code in the same document to produce tables and figures is extremely useful. No more copy/pasting from Excel to Word for tables and figures!\n\n\nSeparate files for each chapter\nLarge word documents can quickly become unwieldy, and increasingly frustrating to work with as the document grows in length. R Markdown lets you easily combine multiple chapters/files into one document when you press knit.\n\n\nLearn a new skill\nI found writing in Markdown really satisfying. Writing a thesis can certainly be a little dull at times, but there were always new things to learn while using Markdown, and this certainly helped keep me motivated when writing wasn‚Äôt going so well‚Ä¶\n\n\n\nPotential downsides\n\nComments\nThis was the only major issue I faced using Markdown. While my supervisors were happy enough for me to use R Markdown, they were not too keen on learning how to use git and start opening pull requests on my draft chapters‚Ä¶ So for feedback, I ended up knitting to .docx so comments could be tracked using Microsoft word. The downside to this was that formatting often went astray, but over time they were ok with this after seeing the aesthetically pleasing PDF outputs that were possible.\n\n\nInevitably you will need to learn some LaTeX\nR markdown does a fantastic job and can get you 95% of the way there without ever having to open a .tex file. However there will likely be some fine tuning that is either not easy or not possible to do with R Markdown, and using If unfamiliar with LaTeX, this can be a steep learning curve, but there are ample resources out there to help figure things out.\n\n\nPotential to get distracted from writing\nOccasionally I would spend time fiddling with the layout of certain pages, or trying to get a specific table looking a certain way, when I really should have just saved that till after the writing was finished. My tip would be dedicate time to writing and time to editing and try not to mix the two. That way you can dedicate your full focus to each task, and not get distracted by one when you need to be working on the other.\n\n\n\nCredits\nBelow are some of the key resources I used during my write up.\n\nR Markdown: Providing the tools Xie, Allaire, and Grolemund (2018)\nBookdown: Excellent package by yihui for writing all kinds of documents Xie (2016) https://github.com/rstudio/bookdown\nThesisdown: Extension to bookdown focusing specifically on thesis style documents https://github.com/ismayc/thesisdown Solomon (2020)\nReally useful blog by Ed Berry I found early on in R markdown journey (Thanks Ed) https://eddjberry.netlify.com/post/writing-your-thesis-with-bookdown/\nAnother useful blog for some additional formatting tips https://rpubs.com/theycke/380678\nPapaja: Fantastic package for formatting in APA style https://github.com/crsh/papaja Aust and Barth (2018)\n\n\n\n\n\n\n\n\n\nReferences\n\nAust, Frederik, and Marius Barth. 2018. Papaja: Create APA Manuscripts with r Markdown. https://github.com/crsh/papaja.\n\n\nSolomon, Nick. 2020. Thesisdown: An Updated r Markdown Thesis Template Using the Bookdown Package.\n\n\nXie, Yihui. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://github.com/rstudio/bookdown.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown."
  },
  {
    "objectID": "posts/2020-11-22-bitmexr-logo/index.html",
    "href": "posts/2020-11-22-bitmexr-logo/index.html",
    "title": "{bitmexr} gets a hex logo!",
    "section": "",
    "text": "A quick google of ‚Äúhow to create R hex logo‚Äù pointed me towards the hexSticker package, which I promptly installed and set to work. After having a look at some of the examples on https://github.com/GuangchuangYu/hexSticker, I opted for a really simple design (I was never much of an artist üòÜ) using the following code:\n\nlibrary(hexSticker)\n\nsticker(\"/home/harry/Desktop/Bitcoin-Logo.png\",\n        package=\"bitmexr\",\n        p_size=20,\n        s_x=1,\n        s_y=0.75,\n        s_width = 0.9,\n        s_height = 0.9,\n        h_fill = \"#000000\",\n        h_color = \"#f2a900\",\n        asp = 0.9,\n        filename=\"hex.png\")\n\nThe end result looks like this\n\nknitr::include_graphics(\"hex.png\")\n\n\n\n\n\n\n\n\nNot particularly imaginative, but I‚Äôm happy with it as a first attempt!"
  },
  {
    "objectID": "posts/2022-02-05-qbr-update/index.html",
    "href": "posts/2022-02-05-qbr-update/index.html",
    "title": "qbr v1.0.0",
    "section": "",
    "text": "Recently I made some major changes to the qbr package I released back in June 2020. As a quick refresher, qbr is an R wrapper around the popular javascript library queryBuilder. In short, it provides a nice UI for constructing queries that can be used to filter data and also plays nicely with Shiny as its two main dependencies are bootstrap and jquery.\nI initially developed qbr using the htmlwidgets framework, guided by an earlier attempt at integrating the library with R by @harveyl888. However after using qbr in a couple of projects at work, I found there were a few limitations, particularly when it came to updating the widget with new filters. So this latest attempt moves away from the htmlwidgets approach and instead uses a custom input binding for Shiny, and implements some additional functionality, all with the aim of making the library easier to use and customise as required."
  },
  {
    "objectID": "posts/2022-02-05-qbr-update/index.html#so-whats-new",
    "href": "posts/2022-02-05-qbr-update/index.html#so-whats-new",
    "title": "qbr v1.0.0",
    "section": "So, whats new?",
    "text": "So, whats new?\nBelow are just a few of the highlights in the new version of qbr - checkout the documentation for more details!\nSupport for different bootstrap versions\nWhile not ‚Äúofficially‚Äù supported by the original js library, some kind folk have made suggested some fixes that enable the library to be used with bootstrap 4+. Within the new useQueryBuilder() function you can set bs_version to one of ‚Äú3‚Äù, ‚Äú4‚Äù or ‚Äú5‚Äù (default is ‚Äú3‚Äù) in order to make sure the builder looks and functions ok with newer versions of boostrap. This is especially useful if you‚Äôre using bslib to update the bootstrap version used by shiny!\nEasily update the builder with updateQueryBuilder()\nThe original library has many methods that can used to on an existing builder. Currently updateQueryBuilder() supports the following,\n\nreset: Removes all rules from the builder,\ndestroy: Self explanatory..!,\nsetRules: Load a predetermined set of rules into the builder.\nsetFilters: Remove all existing filters and add a new set of filters to the builder\naddFilter: Add a filter to the existing filters\n\nI‚Äôve found these to be particularly useful when you need to builder to update depending on the data or a users actions in the app.\nChoose your output\nIn the queryBuilderInput() function, you can use the return_value argument to choose the output that gets returned to shiny. This can be one of:\n\n\"r_rules\": Returns the filter condition formatted for R\n\"sql_rules\": Returns the filter condition formatted for SQL\n\"rules\": list of conditions, default output from original queryBuilder library\n\"all\": Return all of the above in a named list.\n\nThe aim here is to try and give the user more control over what output is returned.\nConditional dependencies for widgets\nqueryBuilder supports many different plugins and widgets that depend on additional dependencies. In the previous version of qbr, all of these dependencies were loaded in when you used the builder, even if you didn‚Äôt use any of the plugins or widgets! This led to a noticeable increase in loading time, and was just unnecessary. Now, only the dependencies for the requested plugins will be loaded, which should improve the loading time of the builder and is generally more efficient.\n\nDemo\nBelow is a demo app using a couple of different configurations available.\n\n\n\n\n\n\nSummary\nSo there it is! I still have a few ideas that I‚Äôd like to implement and generally make the package more robust, but in the mean time I‚Äôd love any feedback on the package, so feel free to drop a comment or open an issue on github. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-10-22-introducing-powerof10/index.html",
    "href": "posts/2020-10-22-introducing-powerof10/index.html",
    "title": "Introducing {poweRof10}.",
    "section": "",
    "text": "Enough chatter, show us the package!\n\npoweRof10\nYou can install from github with:\n\nremotes::install_github(\"hfshr/poweRof10\")\n\n# packages used\nlibrary(poweRof10)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(emo)\nlibrary(lubridate)\n\nThe package is really simple. It has two functions, the first gets the data for an individual athlete, while the second can be used to get the rankings for a given year, event and gender.\nFor individual athletes you can use get_athlete:\n\nme <- get_athlete(fn = \"Harry\", sn = \"Fisher\", club = \"Cardiff\") %>% \n  select(event, perf, pos, venue, date) \n\nWarning: `html_session()` was deprecated in rvest 1.0.0.\nPlease use `session()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nWarning: `submit_form()` was deprecated in rvest 1.0.0.\nPlease use `session_submit()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nIt is then possible to do any number of things with the data such as‚Ä¶\nMost popular event:\n\nme %>% \n  count(event) %>% \n  arrange(-n) %>% \n  head(5) %>% \n  gt() %>% \n  cols_label(\n    n = \"Number of performances\",\n    event = \"Event\"\n  )\n\n\n\n\n  \n  \n    \n      Event\n      Number of performances\n    \n  \n  \n    800\n110\n    400\n31\n    1500\n21\n    600\n2\n    HT6K\n2\n  \n  \n  \n\n\n\n\nClearly the 800m was my preferred event. Although I did take part in some hammer throwing in my early days‚Ä¶ (I was terrible at the hammer throw üòÇ).\nNumber of podium finishes:\n\nme %>% \n  mutate(pos = as.numeric(pos)) %>% \n  filter(pos < 4) %>% \n  count(pos) %>% \n  mutate(medal = c(medal(1), medal(2), medal(3))) %>% \n  select(medal, n) %>% \n  gt() %>% \n  cols_align(\"center\") %>% \n  cols_label(\n    medal = \"Position\",\n    n = \"Count\"\n  )\n\n\n\n\n  \n  \n    \n      Position\n      Count\n    \n  \n  \n    ü•á\n42\n    ü•à\n31\n    ü•â\n23\n  \n  \n  \n\n\n\n\nOr progression over time:\n\nme %>% \n  filter(event == \"800\") %>% \n  mutate(date = as.Date(date, format = \"%d %b %y\"),\n         perf = hms::parse_hms(paste(\"00:\" ,perf))) %>% \n  group_by(year = year(date)) %>% \n  arrange(perf) %>% \n  slice(1) %>% \n  ggplot(aes(x = year, y = perf)) +\n  geom_line() +\n  labs(y = \"Performance\", \n       x = \"Year\",\n       title = \"Performance over time in the 800m\") +\n  theme_light()\n\n\n\n\nThe package also includes get_event, which retrieves the rankings for a specific event, age group, gender and year. For example to get the 100m rankings for 2016 for men you would use:\n\nevent_100 <- get_event(event = \"100\", agegroup = \"ALL\", gender = \"M\", year = 2016, top_n = 20) \n\nMaybe you might want to compare the progression in a particular event over the last decade. We can use get_event with a combination of purrr::map_dfr to loop over several dates in one go.\n\n\nTo speed this up you could potentially use furrr::future_map_dfr\n\ndates <- seq(from = 2013, to = 2020)\n\nrankings <- map_dfr(dates, ~get_event(event =\"100\", \n                                      agegroup = \"ALL\", \n                                      gender = \"M\", \n                                      year = .x, \n                                      top_n = 1))\n\nThrowing together a quick visualisation:\n\n\nI‚Äôve used the great ggtext package here to add some colour to the labels and highlight the fastest time\n\nlibrary(ggtext)\n\nrankings %>% \n  mutate(perf = as.numeric(perf),\n         axislow = min(perf)-min(perf)*0.01,\n         axishigh = max(perf)+max(perf)*0.02) %>% \n  mutate(label = str_c(name, perf, sep = \" - \"),\n         label = case_when(perf == min(perf) ~ paste0(\"<span style='color:#D4AF37'>**\", label, \"**</span>\"),\n                           TRUE ~ label)) %>% \n  {\n  ggplot(., aes(x = input_year, y = perf, \n              group = 1, \n              label = label))+\n  geom_point() +\n  geom_segment(aes(x=input_year, y=perf, xend = input_year, yend =min(perf)-min(perf)*0.01))+\n  scale_y_continuous(expand = c(0,0), limits = c(.$axislow[1], .$axishigh[1])) +\n  coord_flip() +\n  geom_richtext(fill = NA, label.color = NA, aes(hjust = -0.05)) +\n  labs(title = \"Top Performances from 2010 to 2020 in the mens 100M\",\n       x = \"Year\",\n       y = \"Performance\") +\n  theme_light() \n  }\n\n\n\n\nClearly 2020 is the outlier here, no prizes for guessing why‚Ä¶!\n\n\nWrapping up\nSo that‚Äôs a quick tour of poweRof10. Not much too it, but I‚Äôve had some fun playing with this data so thought I‚Äôd share it anyway. Feel free to visit the package repo and raise an issue if something doesn‚Äôt work as expected.\nTill next time!"
  },
  {
    "objectID": "posts/2020-04-13-bitmexr/index.html",
    "href": "posts/2020-04-13-bitmexr/index.html",
    "title": "bitmexr: An R client for BitMEX cryptocurrency exchange.",
    "section": "",
    "text": "While writing my previous post, I was surprised to find that there was no R related package for the cryptocurrency exchange BitMEX. While cryptocurrency itself is a somewhat niche area, BitMEX is one of the largest and most popular exchanges, so I had assumed someone would have already put together something in R for accessing data through BitMEX‚Äôs API. As I could find no such package, I drew inspiration from the few ‚Äòcrypto‚Äô related packages that do exist, and set about creating a package that would provide a R users with a set of tools to obtain historic trade data from the exchange..\nBitMEX has a very detailed API that allows users to perform essentially every action possible on the site through the API. This ranges from simple queries about historic price data through to executing trades on the platform. Initially, I just wanted the package to be able to easily access historic data for research purposes, however it would be relatively straightforward to implement additional features such as executing trades through the API.\nAnd like that, bitmexr was born‚Ä¶\nCurrently you can install bitmexr from github, but hopefully the package will be on CRAN soon."
  },
  {
    "objectID": "posts/2020-04-13-bitmexr/index.html#map_-variants",
    "href": "posts/2020-04-13-bitmexr/index.html#map_-variants",
    "title": "bitmexr: An R client for BitMEX cryptocurrency exchange.",
    "section": "map_* variants",
    "text": "map_* variants\nIn addition to the core functions, the packages contains map_* variants of each function. These functions were implemented to address two restrictions within the API:\n\nThe maximum number of rows per API call is limited to 1000\nThe API is limited to 30 requests within a 60 second period\n\nThe map_* functions are useful for when the data you wanted to return is greater than the 1000 row limit, but you want to avoid running in to the request limit (too many request timeouts may lead to an IP for up to one week).\nFor example, say you want to get hourly bucketed trade data from the 2019-01-01 to 2020-01-01.\n\nbucket_trades(startTime = \"2019-01-01\", \n              endTime = \"2020-01-01\", \n              binSize = \"1h\",\n              symbol = \"XBTUSD\") %>% \n  filter(timestamp == max(timestamp)) %>% \n  select(timestamp) %>% \n  kable()\n\n\n\n\ntimestamp\n\n\n\n\n2019-02-11 15:00:00\n\n\n\n\n\nThe first 1000 rows have only returned data up until 2019-02-11. To obtain the rest of the data, you would need to pass in this start date and run the function again, repeating this process until you had the desired time span of data.\nThis is where the map_* variants come in handy.\n\nmap_bucket_trades(start_date = \"2019-01-01\", \n                  end_date = \"2020-01-01\", \n                  binSize = \"1h\",\n                  symbol = \"XBTUSD\",\n                  verbose = FALSE) %>% \n  paged_table()\n\n\n  \n\n\n\nIf verbose is set to TRUE information about what is going on and a progress bar showing how long is left is printed to the console. Now the end date is what we wanted.\nmap_trades() works in a similar way, however because the number of each trades for a specified time interval is not known in advance, no progress bar is printed. Instead, the last date of the most recent API call is printed to provide an indication of how long is left (relative to the inputted start and end date). This function uses a repeat loop that will keep calling the API until the start date is greater than the end date.\nThe following example gets the every trade for ‚ÄúXBTUSD‚Äù between 12:00 and 12:15 on the 6th June 2019.\n\nmap_trades(symbol = \"XBTUSD\",\n           start_date = \"2019-06-01 12:00:00\",\n           end_date = \"2019-06-01 12:15:00\") %>% \n  select(-trdMatchID) %>% # unique trade identifier, not particularly interesting\n  paged_table()\n\n\n  \n\n\n\nThis short 15 minute time interval resulted in ~6000 trades being returned - consequently this function should only be used for very specific time intervals where you require individual trade data. A warning is given if a time interval of greater than 1 day is provided. For example:\n\ninclude_graphics(\"images/warning.png\")\n\n\n\n\nIn contrast, using bucket_trades() with the smallest binSize setting summarises those 6000 rows into 16 - one for each minute between 12:00 and 12:15.\n\nbucket_trades(symbol = \"XBTUSD\",\n              startTime = \"2019-06-01 12:00:00\",\n              endTime = \"2019-06-01 12:15:00\",\n              binSize = \"1m\") %>% \n  paged_table()"
  },
  {
    "objectID": "posts/2020-04-13-bitmexr/index.html#use-with-other-packages",
    "href": "posts/2020-04-13-bitmexr/index.html#use-with-other-packages",
    "title": "bitmexr: An R client for BitMEX cryptocurrency exchange.",
    "section": "Use with other packages",
    "text": "Use with other packages\nbitmexr simply allows the user to get the data. With data in hand, there are several fantastic packages that can be used to help explore, visualise the data further. Two personal favourites are tidyquant(Dancho and Vaughan 2020) and gganimate(Pedersen and Robinson 2020).\nCombining bitmexr and tidyquant makes it easy to perform financial analysis. For example, comparing the monthly returns of the two most traded contracts on Bitmex\n\nbtc_year <- map_dfr(c(\"XBTUSD\", \"ETHUSD\"), ~map_bucket_trades(symbol = .x,\n                                                              binSize = \"1d\"))\n\nEarliest start date for given symbol is: 2018-08-02 09:06:10.\nContinuing with earliest start date\n\nbtc_year %>% \n  filter(timestamp > \"2018-08-01\") %>% # ETHUSD only available since August 2018\n  group_by(symbol) %>% \n  tq_transmute(select = close,\n               mutate_fun = periodReturn,\n               period = \"monthly\",\n               type = \"log\",\n               col_rename = \"monthly_returns\") %>% \n  ggplot(aes(x = timestamp, y = monthly_returns, fill = symbol)) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_tq()\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\nWarning: `type_convert()` only converts columns of type 'character'.\n- `df` has no columns of type 'character'\n\nWarning: `type_convert()` only converts columns of type 'character'.\n- `df` has no columns of type 'character'\n\n\n\n\n\ngganimate makes it easy to visualise those exciting (or worrisome, depending which side of the trade you were on‚Ä¶!) periods of high volatility.\nA personal favourite was the parabolic rise in late 2017‚Ä¶\n\nvol <- map_bucket_trades(start_date = \"2017-08-05\",\n                         end_date = \"2017-12-25\",\n                         symbol = \"XBTUSD\",\n                         binSize = \"1h\")\n\n\np <- vol %>% \n  filter(timestamp <= \"2017-12-17\") %>% \n  ggplot(aes(x = timestamp, y = close)) +\n  geom_candlestick(aes(open = open, high = high, low = low, close= close),\n                   fill_up = \"green\",\n                   fill_down = \"red\",\n                   colour_up = \"green\",\n                   colour_down = \"red\") +\n  scale_y_continuous(labels = scales::dollar) +\n  transition_time(timestamp) +\n  shadow_mark() +\n  theme_minimal() +\n  view_follow()\n\nanimate(p, end_pause = 5)\n\n\n\n\n‚Ä¶but I‚Äôll spare the pain of showing what came next‚Ä¶!"
  },
  {
    "objectID": "posts/2020-11-30-model-stacking/index.html",
    "href": "posts/2020-11-30-model-stacking/index.html",
    "title": "When one model is not enough: Stacking models with {stacks}.",
    "section": "",
    "text": "The whole is greater than the sum of its parts\n\n‚Ä¶and maybe they were talking about model stacking?\nBesides, to get to the point of this post, I wanted to explore the stacks package (Couch and Kuhn 2020) for creating (unsurprisingly) model stacks. stacks a fairly recent development allowing model stacking to be achieved within the tidymodels ideology. Apart from having a great hex logo, stacks provides some powerful tools to create model stacks, and I‚Äôve included a few notes on my first experiences using the package with hopefully a motivating example..!\n\n\n\n\n\n\n\n\n\n\n\n\nThe problem\nInspired by chapter 10 of Kuhn and Johnson‚Äôs (2013) Applied Predictive Modelling, the problem we‚Äôll be addressing is the compressive strength of different mixture of concrete. Yep that‚Äôs right, this post is going to be about concrete. But wait! I promise this isn‚Äôt as dull as it sounds ü§£.\nFirst lets have a quick look at the data.\n\n# dataset is availble in modeldata package\nlibrary(modeldata)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(stacks)\nlibrary(rules)\nlibrary(see) # for nice theme\n\ndata(\"concrete\")\nskimr::skim(concrete)\n\n\nData summary\n\n\nName\nconcrete\n\n\nNumber of rows\n1030\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncement\n0\n1\n281.17\n104.51\n102.00\n192.38\n272.90\n350.00\n540.0\n‚ñÜ‚ñá‚ñá‚ñÉ‚ñÇ\n\n\nblast_furnace_slag\n0\n1\n73.90\n86.28\n0.00\n0.00\n22.00\n142.95\n359.4\n‚ñá‚ñÇ‚ñÉ‚ñÅ‚ñÅ\n\n\nfly_ash\n0\n1\n54.19\n64.00\n0.00\n0.00\n0.00\n118.30\n200.1\n‚ñá‚ñÅ‚ñÇ‚ñÇ‚ñÅ\n\n\nwater\n0\n1\n181.57\n21.35\n121.80\n164.90\n185.00\n192.00\n247.0\n‚ñÅ‚ñÖ‚ñá‚ñÇ‚ñÅ\n\n\nsuperplasticizer\n0\n1\n6.20\n5.97\n0.00\n0.00\n6.40\n10.20\n32.2\n‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÅ\n\n\ncoarse_aggregate\n0\n1\n972.92\n77.75\n801.00\n932.00\n968.00\n1029.40\n1145.0\n‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÇ\n\n\nfine_aggregate\n0\n1\n773.58\n80.18\n594.00\n730.95\n779.50\n824.00\n992.6\n‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñÅ\n\n\nage\n0\n1\n45.66\n63.17\n1.00\n7.00\n28.00\n56.00\n365.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ncompressive_strength\n0\n1\n35.82\n16.71\n2.33\n23.71\n34.44\n46.14\n82.6\n‚ñÖ‚ñá‚ñá‚ñÉ‚ñÅ\n\n\n\n\n\nWe can also plot the relationship between compressive strength and each of the predictors in the data.\n\nconcrete %>%\n  pivot_longer(-compressive_strength) %>%\n  ggplot(aes(x = value, y = compressive_strength)) +\n  geom_point(alpha = 0.3, size = 1) +\n  geom_smooth() +\n  facet_wrap(~name, scales = \"free_x\") +\n  labs(\n    x = \"Predictor\",\n    y = \"Compressive strength\",\n    title = \"Relationship between predictors and compressive strength\"\n  ) +\n  theme_lucid()\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nFitting the models\nIn chapter 10, Kuhn and Johnson (2013) evaluate a range of models based on data from Yeh (1998) from which I have selected three of the better performing models (which happen to be random forest, neural network and cubist). The goal is to see whether using stacks to create an ensemble of these models will outperform each of the individual models. Quick note, I‚Äôm going to assume some experience with using the tidymodels workflow for modelling to avoid this post become too lengthy. For an introduction to tidymodels, I have a post which covers some of the basics, and you can check out some of the excellent tutorials available on the tidymodels site.\n\n# split the data\nset.seed(1)\nconcrete_split <- initial_split(concrete)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\n# the folds used in tuning steps\nfolds <- rsample::vfold_cv(concrete_train, v = 5)\n\n# basic recipe used in all models\nconcrete_rec <- recipe(\n  compressive_strength ~ .,\n  data = concrete_train\n)\n\n# metric for evaluation\nmetric <- metric_set(rmse, rsq)\n\n# protect your eyes!\noptions(tidymodels.dark = TRUE)\n\n# convenience function\nctrl_grid <- control_stack_grid()\n\n# Basic workflow\ncement_wf <-\n  workflow() %>%\n  add_recipe(concrete_rec)\n\n# random forest #\nrf_spec <-\n  rand_forest(\n    mtry = tune(),\n    min_n = tune(),\n    trees = 500\n  ) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"ranger\")\n\nrf_wflow <-\n  cement_wf %>%\n  add_model(rf_spec)\n\nrf_res <-\n  tune_grid(\n    object = rf_wflow,\n    resamples = folds,\n    grid = 10,\n    control = ctrl_grid\n  )\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n# neural net #\n\nnnet_spec <-\n  mlp(\n    hidden_units = tune(),\n    penalty = tune(),\n    epochs = tune()\n  ) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"nnet\")\n\nnnet_rec <-\n  concrete_rec %>%\n  step_corr(all_predictors()) %>%\n  step_normalize(all_predictors())\n\nnnet_wflow <-\n  cement_wf %>%\n  add_model(nnet_spec) %>%\n  update_recipe(nnet_rec)\n\nnnet_res <-\n  tune_grid(\n    object = nnet_wflow,\n    resamples = folds,\n    grid = 10,\n    control = ctrl_grid\n  )\n\n# Cubist #\n\ncubist_spec <-\n  cubist_rules(\n    committees = tune(),\n    neighbors = tune(),\n    max_rules = tune()\n  )\n\ncubist_wflow <-\n  cement_wf %>%\n  add_model(cubist_spec)\n\ncubist_res <-\n  tune_grid(\n    object = cubist_wflow,\n    resamples = folds,\n    grid = 10,\n    control = ctrl_grid\n  )\n\nSo at this point we have fitted 30 models, 10 models for each type (random forest, neural net and cubist). We can do a quick check of how well each of these models performed. For convenience, I‚Äôve created a simple function called finaliser() that selects the best model, updates the workflow, fits the final model with the best parameters and pulls out the metrics.\n\nfinaliser <- function(tuned, wkflow, split, model) {\n  best_mod <- tuned %>%\n    select_best(\"rmse\")\n\n  final_wf <- wkflow %>%\n    finalize_workflow(best_mod)\n\n  final_fit <-\n    final_wf %>%\n    last_fit(split)\n\n  final_fit %>%\n    collect_metrics() %>%\n    mutate(model = model)\n}\n\nbind_rows(\n  finaliser(cubist_res, cubist_wflow, concrete_split, \"cubist\"),\n  finaliser(nnet_res, nnet_wflow, concrete_split, \"nnet\"),\n  finaliser(rf_res, rf_wflow, concrete_split, \"rf\")\n) %>%\n  select(model, .metric, .estimate) %>%\n  pivot_wider(names_from = .metric, values_from = .estimate) %>%\n  arrange(rmse)\n\n# A tibble: 3 √ó 3\n  model   rmse   rsq\n  <chr>  <dbl> <dbl>\n1 cubist  4.54 0.926\n2 rf      5.35 0.899\n3 nnet    5.44 0.894\n\n\nWe can see that the cubist model has the best performance, closely followed by the random forest with the neural net bringing up the rear.\n\n\nTime to stack\nNow we can start stacking! We start by initialising the stack with stacks() then add each candidate model with add_candidates(). Next we evaluate the candidate models with blend_predictions(), before finally training the non-zero members on the training data with fit_members().\n\ncement_st <-\n  # initialize the stack\n  stacks() %>%\n  # add each of the models\n  add_candidates(rf_res) %>%\n  add_candidates(nnet_res) %>%\n  add_candidates(cubist_res) %>%\n  blend_predictions() %>% # evaluate candidate models\n  fit_members() # fit non zero stacking coefficients\n\nLet‚Äôs have a look at our model stack\n\ncement_st\n\n‚îÄ‚îÄ A stacked ensemble model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nOut of 30 possible candidate members, the ensemble retained 10.\nPenalty: 0.01.\nMixture: 1.\n\nThe 10 highest weighted members are:\n\n\n# A tibble: 10 √ó 3\n   member          type          weight\n   <chr>           <chr>          <dbl>\n 1 cubist_res_1_10 cubist_rules 0.375  \n 2 rf_res_1_04     rand_forest  0.296  \n 3 cubist_res_1_07 cubist_rules 0.181  \n 4 cubist_res_1_09 cubist_rules 0.0733 \n 5 nnet_res_1_09   mlp          0.0353 \n 6 nnet_res_1_07   mlp          0.0337 \n 7 nnet_res_1_03   mlp          0.0233 \n 8 cubist_res_1_08 cubist_rules 0.00872\n 9 nnet_res_1_06   mlp          0.00461\n10 nnet_res_1_05   mlp          0.00454\n\n\nOut of the 30 models we initially trained, 7 models had non-zero stacking coefficients and were retained for our model stack. stacks provides a nice autoplot feature that allows us to quickly visualise each of the model members with their weights that are used to make predictions.\n\nautoplot(cement_st, type = \"weights\") +\n  theme_lucid()\n\n\n\n\n\n\nSo, was it worth it?!\nLets first have a look at the predictions made by our stack.\n\n# get predictions with stack\ncement_pred <- predict(cement_st, concrete_test) %>%\n  bind_cols(concrete_test)\n\nggplot(cement_pred, aes(x = compressive_strength, y = .pred)) +\n  geom_point(alpha = 0.4) +\n  coord_obs_pred() +\n  labs(x = \"Observed\", y = \"Predicted\") +\n  geom_abline(linetype = \"dashed\") +\n  theme_lucid()\n\n\n\n\nApart from a handful of points, our stacked model looks like it has done pretty well! We can also see how each of the members in the stack performed by using members = TRUE in the prediction call.\n\nmember_preds <- predict(cement_st, concrete_test, members = TRUE) %>%\n  bind_cols(\n    .,\n    concrete_test %>%\n      select(compressive_strength)\n  ) %>%\n  select(compressive_strength, .pred, everything())\n\nTo visualise this, I‚Äôve selected the first 10 observations, and plotted the residuals. Points on the dashed line are closer to the true value.\n\nplot_preds <- member_preds %>%\n  slice(1:10) %>%\n  rowid_to_column(\"obs\") %>%\n  mutate(obs = factor(obs)) %>%\n  pivot_longer(cols = c(-obs,-compressive_strength), names_to = \"model\", values_to = \"value\") %>% \n  mutate(diff = compressive_strength - value,\n         model = ifelse(model == \".pred\", \"model_stack\", model)\n         )\n\n\nplot_preds %>%\n  filter(model != \"model_stack\") %>% \n  ggplot(aes(x = obs, y = diff, colour = model)) +\n  geom_point(alpha = 0.8) +\n  geom_jitter(width = 0.20) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  viridis::scale_colour_viridis(discrete = T, option = \"C\") +\n  labs(y = \"Residual\", subtitle = paste(\"Model stack predictions marked with\", emo::ji(\"x\"))) +\n  geom_point(data = plot_preds %>% \n               filter(model == \"model_stack\"), colour = \"red\", shape = 4, size = 5) +\n  theme_lucid()\n\n\n\n\nApart from the first observation where none of the models performed particularly well, you can see how different models resulted in different predictions with some performing better than others.\nNow what we‚Äôre really interested in is if the model stack performed better than any of the individual models. To determine this, lets look at some metrics:\n\nmulti_metric <- metric_set(rmse, rsq)\n\nmap_dfr(\n  member_preds,\n  ~ multi_metric(\n    member_preds,\n    truth = compressive_strength,\n    estimate = .x\n  ),\n  .id = \"model\"\n) %>%\n  select(model, .metric, .estimate) %>%\n  pivot_wider(names_from = .metric, values_from = .estimate) %>%\n  filter(model != \"compressive_strength\") %>%\n  mutate(model = if_else(model == \".pred\", \"model_stack\", model)) %>%\n  arrange(rmse) %>% \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 11 √ó 3\n   model            rmse   rsq\n   <chr>           <dbl> <dbl>\n 1 model_stack      4.65  0.92\n 2 cubist_res_1_07  4.67  0.92\n 3 cubist_res_1_08  4.75  0.92\n 4 cubist_res_1_10  4.86  0.92\n 5 cubist_res_1_09  5.01  0.91\n 6 rf_res_1_04      5.41  0.9 \n 7 nnet_res_1_03    5.69  0.88\n 8 nnet_res_1_09    5.79  0.88\n 9 nnet_res_1_05    5.91  0.88\n10 nnet_res_1_07    7.52  0.8 \n11 nnet_res_1_06    9.38  0.68\n\n\nWe can see the model stack performs better than any of the individual models for both the rmse and r-squared metrics, which is pretty cool! The cubist models are clearly the strongest but the model stack that includes the inputs from the random forest and neural nets as well as cubist edges slightly ahead in these metrics.\n\n\nSummary\nSo that was a quick tour of the stacks package. I‚Äôd highly recommend checking out the package website which has lots of good examples on which this post was heavily inspired.\nThanks for reading!\n\n\n\n\n\nReferences\n\nCouch, Simon, and Max Kuhn. 2020. Stacks: Tidy Model Stacking. https://CRAN.R-project.org/package=stacks.\n\n\nKuhn, Max, and Kjell Johnson. 2013. ‚ÄúApplied Predictive Modeling.‚Äù New York, NY: Springer. 2013. http://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/.\n\n\nYeh, I C. 1998. ‚ÄúModeling of Strength of High-Performance Concrete Using Artificial Neural Networks.‚Äù Cement and Concrete Research 28 (12). https://doi.org/10.1016/S0008-8846(98)00165-3."
  },
  {
    "objectID": "posts/2020-03-22-bitcoin/index.html",
    "href": "posts/2020-03-22-bitcoin/index.html",
    "title": "Exploring the recent Bitcoin crash with {tidyquant} and {gganimate}.",
    "section": "",
    "text": "Getting the data\nTo obtain the necessary data, I first defined a quick function based on the information found on the BitMEX API explorer page (https://www.bitmex.com/api/explorer). This function barely scratches the surface at the possible requests available - but does get the data needed.\n\nxbt_reader <- function(symbol = \"XBT\", \n                       timeframe = \"1d\", \n                       count = \"1000\", \n                       starttime = \"\",\n                       reverse = 'false'){\n  \n  base <- \"https://www.bitmex.com/api/v1/trade/bucketed?\"\n  symbol = symbol\n  timeframe = timeframe\n  count = count\n  starttime = starttime\n  \n  url <- paste0(base, \n                'binSize=', timeframe, \n                '&partial=false&symbol=', symbol, \n                '&count=', count, \n                '&reverse=', reverse,\n                '&startTime=', starttime)\n  \n result <- tibble(data = content(GET(url), \"parsed\")) %>%\n    unnest_wider(data)\n}\n\nNext, I used this function to get the bucketed 5 minute trade data starting from the day before the crash.\n\nxbt <- xbt_reader(timeframe = \"5m\", starttime = \"2020-03-11\") %>% \n  mutate(timestamp = as_datetime(timestamp))\n\n\n\nVisualising the data\nThe price action can now be plotted using a combination of tidyqaunt and gganimate.\n\nxbt %>% \n  mutate(label = \"help\") %>% \n  ggplot(aes(x = timestamp, y = close)) +\n  geom_candlestick(aes(open = open, high = high, low = low, close= close),\n                   fill_up = \"green\",\n                   fill_down = \"red\",\n                   colour_up = \"green\",\n                   colour_down = \"red\") +\n  scale_x_datetime(date_breaks = \"12 hour\", date_labels = \"%H%M\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(y = \"Price (USD)\", \n       x = \"Time\")  +\n  transition_time(timestamp) +\n  shadow_mark() +\n  theme_minimal()\n\n\n\n\nAs can be seen, this was a dramatic sell of with price dropping from almost 8000 USD to the low 3000 USD mark within a matter of hours, before recovery slightly to around the 5000 USD level.\n\n\nDDoS attack\nWhat makes this price action all the more intriguing is the reported DDoS attacks suffered by Bitmex on the 13th March. Bitmex reported two attacks, taking place at 02:16 UTC and 12:56 UTC. The first attack also happened to coincide with the lowest price during the drop which has caused some suspicion giving the timing of the attacks‚Ä¶ However, speculation aside, these attacks can be clearly visualised using the trade data.\n\n\nMore information on the DDoS attacks can be found here: https://blog.bitmex.com/how-we-are-responding-to-last-weeks-ddos-attacks\nI‚Äôve adjusted the time interval to the lowest resolution possible (1 minute) to clearly see the effect of the DDoS attacks.\n\nxbt_lower <- xbt_reader(timeframe = \"1m\", starttime = \"2020-03-13\") %>% \n  mutate(timestamp = as_datetime(timestamp)) %>% \n  filter(timestamp > \"2020-03-13 11:30:00\" & timestamp < \"2020-03-13 13:30:00\" |\n        timestamp > \"2020-03-13 01:30:00\" & timestamp < \"2020-03-13 03:30:00\") %>% \n  mutate(ddos = case_when(timestamp > \"2020-03-13 01:30:00\" & timestamp < \"2020-03-13 03:30:00\" ~ \"First attack\",\n                          timestamp > \"2020-03-13 11:30:00\" & timestamp < \"2020-03-13 13:30:00\" ~ \"Second attack\",\n                          )) \n\n\np1 <- xbt_lower %>% \n  ggplot(aes(x = timestamp, y = close)) +\n  geom_candlestick(aes(open = open, high = high, low = low, close= close),\n                   fill_up = \"green\",\n                   fill_down = \"red\",\n                   colour_up = \"green\",\n                   colour_down = \"red\")  +\n  facet_wrap(~ddos, scales = \"free\") +\n  scale_y_continuous(labels = scales::dollar)+\n  labs(x = \"\", y = \"Price (USD)\") +\n  theme_minimal()\n\n\n\np2 <- xbt_lower %>% \n  ggplot(aes(timestamp, volume/1000)) +\n  geom_col() +\n  facet_wrap(~ddos, scales = \"free\") +\n  theme_minimal()  +\n  labs(x = \"Time\", y = \"Volume\") +\n  theme(\n    strip.background = element_blank(),\n    strip.text.x = element_blank()\n  )\n\n\np1 / p2 +\n  plot_annotation(title = \"Price data during the DDoS attacks\")\n\n\n\n\nThe effect of each DDoS is clearly visible from the price data, leaving large gaps for approximately 15 minutes where very few to no trades were placed. Immediately following each attack was a period of extreme volatility, before price was able to stabilise to some degree.\nThese conditions would have expectationally difficult for many traders, although not all together unpredictable given the highly volatile nature of Bitcoin and the risk that comes with it‚Ä¶!\n\n\nSummary\nThere are a several other avenues that could be interesting to explore here, such as visualising the order book liquidity during the crash compared to a more ‚Äústable‚Äù period of time, however I may save that for a follow-up post. I have also been playing with the Bitmex websocket connection which streams real time data from the exchange - I think a shiny app would be really cool to visualise this so may try something there as well.\nThanks for reading!\n\n\nReferences\n\n\n\n\n\nReferences\n\nDancho, Matt, and Davis Vaughan. 2020. Tidyquant: Tidy Quantitative Financial Analysis. https://CRAN.R-project.org/package=tidyquant.\n\n\nPedersen, Thomas Lin, and David Robinson. 2020. Gganimate: A Grammar of Animated Graphics. https://CRAN.R-project.org/package=gganimate."
  },
  {
    "objectID": "posts/2020-05-02-pretty-tables-with-gt/index.html",
    "href": "posts/2020-05-02-pretty-tables-with-gt/index.html",
    "title": "Pretty tables with {gt}.",
    "section": "",
    "text": "gt (Iannone, Cheng, and Schloerke 2020) has been under development for a while, but a stable version has recently been released to CRAN. In order to try the package out we first need some data for creating tables. I‚Äôll be using some Bitcoin price data (shameless self plug for my bitmexr (Fisher 2020) package that has just landed on CRAN - check it out here).\nFor demonstration purposes, I pulled the OHLC prices for the last 10 days.\nThe most basic table without any formatting applied looks like this:\nTo get started, I borrowed some ideas straight from an example on https://gt.rstudio.com/ and applied some simple formatting to the raw data in the table.\nStarting to look a little more presentable. Next, I\nA cool feature of gt is that you can rename the column labels to something different, but refer to the original labels for any further manipulation.\nFinally, it is also very easy to add custom html to the tables using the text_transformations() function (again inspired by https://gt.rstudio.com/). The rocket and sad face indicate whether the closing price was greater or less than the opening price for each dat\nLooks good! I‚Äôve barely scratched the surface of what gt can do here and the package website is well worth a look if you are interested."
  },
  {
    "objectID": "posts/2020-05-02-pretty-tables-with-gt/index.html#bonus-table",
    "href": "posts/2020-05-02-pretty-tables-with-gt/index.html#bonus-table",
    "title": "Pretty tables with {gt}.",
    "section": "Bonus table",
    "text": "Bonus table\nOne experimental feature in gt is the ability to add a ggplot (or any image for that matter) to a table. Yes thats right, a ggplot in a table. I did have to do a bit of rummaging in the package repo to get this to work, but the idea is really cool.\nTo demonstrate this functionality I pulled some new Bitcoin price data - the hourly OHLC for the whole on April 2020. The goal is to summarise this information for weekly intervals, with the total volume and trades and a plot of the price action for that week.\nFirst I define a few functions that will help in creating the table/plot combo.\n\n# helper for summary data\noc <- function(name, df) {\n  df %>%\n    summarise(\n      open = dplyr::first(open),\n      close = dplyr::last(close),\n      volume = sum(volume),\n      trades = sum(trades),\n      date = dplyr::first(timestamp)\n    )\n}\n\n# ggplot for each week\nplot_group <- function(name, df) {\n  plot_object <-\n    ggplot(\n      data = df,\n      aes(x = timestamp, y = close)\n    ) +\n    geom_barchart(\n      aes(open = open, high = high, low = low, close = close),\n      fill_up = \"green\",\n      fill_down = \"red\",\n      colour_up = \"green\",\n      colour_down = \"red\"\n    ) +\n    theme_minimal() +\n    theme(\n      legend.position = \"none\",\n      axis.title = element_blank(),\n      axis.text = element_blank()\n    )\n  return(plot_object)\n}\n\n# combine ggplot within table\nfmt_ggplot <- fmt_gg <- function(\n  data,\n  columns,\n  rows = NULL,\n  height = 100,\n  aspect_ratio = 1.0\n) {\n  rows <- rlang::enquo(rows)\n\n  fmt(\n    data = data,\n    columns = columns,\n    rows = !!rows,\n    fns = list(\n      html = function(x) {\n        map(\n          x,\n          ggplot_image,\n          height = height,\n          aspect_ratio = aspect_ratio\n        )\n      }\n    )\n  )\n}\n\n\nThe first is just a simple helper function which generates some summary information for the final table\nThe second is the ggplot object that will be used to visualise each weeks data.\nThe third was borrowed straight from gt‚Äôs repo here. I think some of this functionality is still under development, so is not yet available in the CRAN release.\n\n2022 update: It is now much easier to support ggplot objects in gt tables! See here\nWe‚Äôre now ready to prepare the data for the table.\n\nnewdata <- map_bucket_trades(\n  start_date = \"2020-04-01\",\n  end_date = \"2020-04-28\",\n  binSize = \"1h\"\n) %>%\n  mutate(week = lubridate::week(timestamp)) %>%\n  select(week, timestamp, open, high, low, close, volume, trades) %>%\n  group_by(week) %>%\n  nest() %>%\n  mutate(\n    plot = map2(week, data, plot_group),\n    info = map2(week, data, oc)\n  ) %>%\n  unnest(info)\n\nhead(newdata)\n\n# A tibble: 4 √ó 8\n# Groups:   week [4]\n   week data               plot    open close  volume trades date               \n  <dbl> <list>             <list> <dbl> <dbl>   <dbl>  <int> <dttm>             \n1    14 <tibble [168 √ó 7]> <gg>   6440. 7166  1.45e10 5.35e6 2020-04-01 00:00:00\n2    15 <tibble [168 √ó 7]> <gg>   7166  6880. 1.21e10 4.67e6 2020-04-08 00:00:00\n3    16 <tibble [168 √ó 7]> <gg>   6880. 6898. 1.20e10 4.24e6 2020-04-15 00:00:00\n4    17 <tibble [145 √ó 7]> <gg>   6898. 7788  9.93e 9 4.28e6 2020-04-22 00:00:00\n\n\nNow the data is ready, we can make the table.\n\nnewdata %>%\n  ungroup() %>%\n  select(date, volume, trades, open, close, plot) %>%\n  mutate(date = as.character(date)) %>%\n  gt() %>%\n  text_transform(\n    locations = cells_body(columns = plot),\n    fn = function(x) {\n      newdata$plot %>% \n        ggplot_image(height = px(200))\n    }\n  ) %>%\n  fmt_number(\n    columns = c(trades, volume),\n    suffixing = TRUE\n  ) %>%\n  fmt_date(\n    columns = c(date),\n    date_style = 8\n  ) %>%\n  cols_align(\n    align = \"center\"\n  ) %>%\n  fmt_currency(\n    columns = c(open, close)\n  ) %>%\n  tab_header(\n    title = md(\"**Price action summary for April 2020**\")\n  ) %>%\n  cols_label(\n    date = \"Week beginning\",\n    open = \"Open\",\n    close = \"Close\",\n    trades = \"Trades\",\n    volume = \"Volume\",\n    plot = \"Price action\"\n  ) %>%\n  tab_spanner(\n    label = \"Summary\",\n    columns = c(volume, trades, open, close)\n  )\n\n\n\n\n  \n    \n      Price action summary for April 2020\n    \n    \n  \n  \n    \n      Week beginning\n      \n        Summary\n      \n      Price action\n    \n    \n      Volume\n      Trades\n      Open\n      Close\n    \n  \n  \n    1 April 2020\n14.53B\n5.35M\n$6,440.50\n$7,166.00\n\n    8 April 2020\n12.10B\n4.67M\n$7,166.00\n$6,880.50\n\n    15 April 2020\n12.03B\n4.24M\n$6,880.50\n$6,897.50\n\n    22 April 2020\n9.93B\n4.28M\n$6,897.50\n$7,788.00\n\n  \n  \n  \n\n\n\n\nPretty cool! The possibilities are endless with this and it was good fun trying to get it to work."
  },
  {
    "objectID": "posts/2020-06-07-variable-importance-with-fastshap/index.html",
    "href": "posts/2020-06-07-variable-importance-with-fastshap/index.html",
    "title": "Opening the black box: Exploring xgboost models with {fastshap} in R",
    "section": "",
    "text": "While maximising a models performance is often desirable, it can sometimes limit the explainability. Being able to understand why your model is making certain predictions is vital if the model is going to be used to make important business decision that will need to be explained. This post is going to explore how we can use SHapley Additive exPlanations (SHAP) to dig a little deeper into complex models in an attempt to understand why certain predictions are made."
  },
  {
    "objectID": "posts/2020-06-07-variable-importance-with-fastshap/index.html#variable-importance",
    "href": "posts/2020-06-07-variable-importance-with-fastshap/index.html#variable-importance",
    "title": "Opening the black box: Exploring xgboost models with {fastshap} in R",
    "section": "Variable importance",
    "text": "Variable importance\nBefore getting to SHAP, we can do a quick check of what variables are most important. The vip package is an excellent choice for this, providing a ‚Äúmodel agnostic‚Äù approach to assess variable importance (Greenwell, Boehmke, and Gray 2020).\n\nlibrary(vip)\n\n# Get our model object\nxg_mod <- pull_workflow_fit(xgboost_wflow)\n\nvip(xg_mod$fit)\n\n\n\n\nThis gives us a good first insight into what variables are contributing the most within the model. ‚ÄúIncome‚Äù and ‚ÄúPrice‚Äù appear to be strong predictors in the model, but we can dig a little deeper with fastshap."
  },
  {
    "objectID": "posts/2020-06-07-variable-importance-with-fastshap/index.html#fastshap",
    "href": "posts/2020-06-07-variable-importance-with-fastshap/index.html#fastshap",
    "title": "Opening the black box: Exploring xgboost models with {fastshap} in R",
    "section": "Fastshap",
    "text": "Fastshap\nFor a brief introduction to SHAP, Scott Lundberg (developer of the SHAP approach and shap python package) has a great talk here that gives a shortish (~18mins) overview of the main concepts. You can also review the paper (Lundberg and Lee 2017) for a more in-depth look into the theory underpinning SHAP. As a very high level explanation, the SHAP method allows you to see what features in the model caused the predictions to move above or below the ‚Äúbaseline‚Äù prediction. Importantly this can be done on a row by row basis, enabling insight into any observation within the data.\nWhile there a a couple of packages out there that can calculate shapley values (See R packages iml and iBreakdown; python package shap), the fastshap package (Greenwell 2020) provides a fast (hence the name!) way of obtaining the values and scales well when models become increasingly complex. Below, we‚Äôll walk through some of the main functions in the package and how they can help aid explanations.\n\n\nYou can actually access fastshap directly from the vip package using vip::vi_shap() which uses fastshap under the hood.\nFirst, we need to supply the fastshap::explain() function with the model and the features we used to train the model. As we used some preprocessing steps, we‚Äôll need to prep and juice our training data to ensure it is the same as the data that was used in the model.\n\nlibrary(fastshap)\n\n\nAttaching package: 'fastshap'\n\n\nThe following object is masked from 'package:vip':\n\n    gen_friedman\n\n\nThe following object is masked from 'package:dplyr':\n\n    explain\n\n# Apply the preprocessing steps with prep and juice to the training data\nX <- prep(rec, train) %>% \n  juice() %>% \n  select(-Status) %>% \n  as.matrix()\n\n# Compute shapley values \nshap <- explain(xg_mod$fit, X = X, exact = TRUE)\n\nWith our shapley values calculated, we can explore the values in several ways. fastshap has a great autoplot ability to quickly visualise the different plots available.\nShapley importance\n\nautoplot(shap)\n\n\n\n\nInterestingly, ‚ÄúAmount‚Äù is clearly the most important feature when using shapely values, whereas it was only the 4th most important when using xgboost importance in our earlier plot.\nDependence plot\nWe can focus on on attributes by using a dependence plot. This allows us to see the relationship between shapely values and a particular feature.\n\n# Create a dataframe of our training data\nfeat <- prep(rec, train) %>% \n  juice()\n\nautoplot(shap, \n         type = \"dependence\", \n         feature = \"Amount\", \n         X = feat,\n         smooth = TRUE, \n         color_by = \"Status\")\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nContribution plots\nContribution plots provide and insight into individual predictions. I‚Äôve identified two extreme cases where the prediction probability is almost 100% for each class:\n\npredict(xgboost_wflow, train, type = \"prob\") %>% \n  rownames_to_column(\"rowid\") %>% \n  filter(.pred_bad == min(.pred_bad) | .pred_bad == max(.pred_bad)) %>% \n  gt()%>% \n  fmt_number(columns = 2:3,\n             decimals = 3) \n\n\n\n\n  \n  \n    \n      rowid\n      .pred_bad\n      .pred_good\n    \n  \n  \n    450\n0.999\n0.001\n    2871\n0.000\n1.000\n  \n  \n  \n\n\n\n\nWe can visualise what features made these extreme predictions like so:\n\nlibrary(patchwork)\np1 <- autoplot(shap, type = \"contribution\", row_num = 1541) +\n  ggtitle(\"Likely bad\")\n\np2 <- autoplot(shap, type = \"contribution\", row_num = 1806) +\n  ggtitle(\"Likely good\")\n\np1+p2\n\n\n\n\nIn the ‚Äúlikely bad‚Äù case, we can see ‚ÄúIncome‚Äù and ‚ÄúAmount‚Äù having a negative impact on prediction, whereas in the ‚Äúlikely good‚Äù case, ‚ÄúAmount‚Äù and ‚ÄúSeniority‚Äù having a positive impact. However, these plots still are not telling us why these features had the impact they did.\n\n\nYou can of course recreate these plots from the original explain() output without using autoplot if needed.\nEnter Force plots.\nAn extension of this type of plot is the visually appealing ‚Äúforce plot‚Äù as shown here and in Lundberg et al. (2018). With reticulate installed, fastshap uses the python shap package under the hood to replicate these plots in R. What these plots show is how different features contribute to moving the predicted value away from the ‚Äúbaseline‚Äù value. The baseline being the average of all predictions (Note: in this case, the baseline score is the average probability of the ‚Äúgood‚Äù class).\n\n\nI had to stretch these out so they didn‚Äôt get squished when rendering the markdown document‚Ä¶\nLikely bad\n\nforce_plot(object = shap[1541,], \n           feature_values = X[1541,], \n           display = \"html\", \n           link = \"logit\")\n\nUsing shap version 0.38.1.\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\nOur bad example shows the features and specific values that move the predicted probability lower from the baseline probability. The combination of a relatively low income and high loan amount seem to indicate a much higher probability of a ‚Äúbad‚Äù outcome (or in this case a lower probability of ‚Äúgood‚Äù outcome).\nLikely good\n\nforce_plot(object = shap[1806,], \n           feature_values = X[1806,], \n           display = \"html\", \n           link = \"logit\") \n\nUsing shap version 0.38.1.\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\nIn the good example, ‚ÄúAmount‚Äù and ‚ÄúSeniority‚Äù act to increase the probably of a ‚Äúgood‚Äù outcome.\nA final approach we can use is to pass multiple values into the force_plot() function. By taking a selection of observations, rotating them 90 degrees and stacking them horizontally, it is possible view explanations for multiple observations. Here I‚Äôve just taken the first 50 values 1. The plot is also interactive, so you can explore the effects of each different features across the 50 samples.\n\nforce_plot(object = shap[c(1:50),], \n           feature_values = X[c(1:50),], \n           display = \"html\", \n           link = \"logit\") \n\nUsing shap version 0.38.1.\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written."
  },
  {
    "objectID": "posts/2020-06-07-variable-importance-with-fastshap/index.html#summary",
    "href": "posts/2020-06-07-variable-importance-with-fastshap/index.html#summary",
    "title": "Opening the black box: Exploring xgboost models with {fastshap} in R",
    "section": "Summary",
    "text": "Summary\nSo that was a quick look at the excellent fastshap package and what it has to offer. I‚Äôm still learning the ins and outs of SHAP this was by no means a comprehensive overview of the topic. As models become increasingly complex, the tools to help explain them become even more important and SHAP seems to provides a great way to shine a light into the ‚Äúblack box‚Äù of the inner workings of complex models.\nAny feedback is more than welcome and thanks for reading!"
  },
  {
    "objectID": "posts/2020-12-26-powerof10-covid/index.html",
    "href": "posts/2020-12-26-powerof10-covid/index.html",
    "title": "How has Covid-19 affected UK athletics rankings in 2020?",
    "section": "",
    "text": "Large drop in the number of athletes on the rankings for 2020.\nElite middle distance runners had comparable performances to previous years, sprint performances were noticeably slower.\nSub-elite athletes generally had slower performances for all events in 2020 compared to previous years.\nA mix of lockdown, fewer opportunities to compete and other Covid-related factors likely contributed to the slower performances."
  },
  {
    "objectID": "posts/2020-12-26-powerof10-covid/index.html#introduction",
    "href": "posts/2020-12-26-powerof10-covid/index.html#introduction",
    "title": "How has Covid-19 affected UK athletics rankings in 2020?",
    "section": "Introduction",
    "text": "Introduction\nCovid-19 has obviously had a hugely detrimental effect on a wide range of sports this year, and at the time of writing still continues to do so. In this post, I wanted to investigate how Covid has impacted track performances in the UK compared to previous seasons. Specifically, I was interested in how the distribution of performances may differ between this season and previous seasons. I also wondered if there may be some differences between the elite athletes who may have had additional opportunities to compete this year (national championships, diamond leagues etc) compared to non-elite athletes, who may have faced the most significant reductions in competitive opportunities.\n\nThe data\nI used my poweRof10 package to collect data from www.thepowerof10.info. Specifically, I gathered the rankings for each season from 2016 to 2020 for both men and women for the following events; 100, 200, 400, 800, 1500 and 5000. If interested, you can see the code I used to collect and prepare the data by expanding the code label below.\n\n\nCode\nknitr::opts_chunk$set(echo = FALSE, message = FALSE)\nlibrary(poweRof10)\nlibrary(tidyverse)\nlibrary(furrr)\nlibrary(lubridate)\nlibrary(hms)\nlibrary(ggridges)\nlibrary(ggrepel)\nlibrary(see)\nlibrary(patchwork)\nlibrary(ggtext)\nlibrary(extrafont)\nlibrary(reactable)\n\n# function from https://fishandwhistle.net/post/2018/modifying-facet-scales-in-ggplot2/ for tweaking facets\nscale_override <- function(which, scale) {\n  if(!is.numeric(which) || (length(which) != 1) || (which %% 1 != 0)) {\n    stop(\"which must be an integer of length 1\")\n  }\n  \n  if(is.null(scale$aesthetics) || !any(c(\"x\", \"y\") %in% scale$aesthetics)) {\n    stop(\"scale must be an x or y position scale\")\n  }\n  \n  structure(list(which = which, scale = scale), class = \"scale_override\")\n}\n\nCustomFacetWrap <- ggproto(\n  \"CustomFacetWrap\", FacetWrap,\n  init_scales = function(self, layout, x_scale = NULL, y_scale = NULL, params) {\n    # make the initial x, y scales list\n    scales <- ggproto_parent(FacetWrap, self)$init_scales(layout, x_scale, y_scale, params)\n    \n    if(is.null(params$scale_overrides)) return(scales)\n    \n    max_scale_x <- length(scales$x)\n    max_scale_y <- length(scales$y)\n    \n    # ... do some modification of the scales$x and scales$y here based on params$scale_overrides\n    for(scale_override in params$scale_overrides) {\n      which <- scale_override$which\n      scale <- scale_override$scale\n      \n      if(\"x\" %in% scale$aesthetics) {\n        if(!is.null(scales$x)) {\n          if(which < 0 || which > max_scale_x) stop(\"Invalid index of x scale: \", which)\n          scales$x[[which]] <- scale$clone()\n        }\n      } else if(\"y\" %in% scale$aesthetics) {\n        if(!is.null(scales$y)) {\n          if(which < 0 || which > max_scale_y) stop(\"Invalid index of y scale: \", which)\n          scales$y[[which]] <- scale$clone()\n        }\n      } else {\n        stop(\"Invalid scale\")\n      }\n    }\n    \n    # return scales\n    scales\n  }\n)\n\nfacet_wrap_custom <- function(..., scale_overrides = NULL) {\n  # take advantage of the sanitizing that happens in facet_wrap\n  facet_super <- facet_wrap(...)\n  \n  # sanitize scale overrides\n  if(inherits(scale_overrides, \"scale_override\")) {\n    scale_overrides <- list(scale_overrides)\n  } else if(!is.list(scale_overrides) || \n            !all(vapply(scale_overrides, inherits, \"scale_override\", FUN.VALUE = logical(1)))) {\n    stop(\"scale_overrides must be a scale_override object or a list of scale_override objects\")\n  }\n  \n  facet_super$params$scale_overrides <- scale_overrides\n  \n  ggproto(NULL, CustomFacetWrap,\n    shrink = facet_super$shrink,\n    params = facet_super$params\n  )\n}\n\n\n### get data ###\nyears <- c(2016:2020)\nevents <- c(\"100\", \"200\", \"400\", \"800\", \"1500\", \"5000\")\ngender <- c(\"M\", \"W\")\n\n# grid <- expand_grid(years, events, gender) %>%\n#   as.list()\n# \n# plan(multisession)\n# all_rankings <- future_pmap_dfr(grid, ~get_event(event = ..2, gender = ..3, year = ..1), .progress = TRUE)\n# plan(sequential)\n# save(all_rankings, file = \"rankings.rds\")\noptions(digits.secs=2)\n\nload(\"rankings.rds\")\n\n# Function to get womens 400 performances in 61.00 format to 1:01.00 format \n\ntime_cleaner <- function(time) {\n  date <- as.numeric(time) * 1000\n  res <- as.POSIXct.numeric(date/1000, origin = '1970-01-01', format = \"%OS\")\n  format(res, format = '%M:%OS')\n}  \n\nclean_rankings <- all_rankings %>% \n  select(rank, perf, input_year, event, gender, date, name) %>% \n  filter(event %in% c(\"100\", \"200\", \"400\", \"800\", \"1500\", \"5000\")) %>% \n  filter(rank != \"\") %>%\n  mutate(event = factor(event, levels = c(\"100\", \"200\", \"400\", \"800\", \"1500\", \"5000\")),\n         rank = as.numeric(rank),\n         perf = str_trim(perf),\n         input_year = factor(input_year, levels = c(\"2016\", \"2017\", \"2018\", \"2019\", \"2020\"))) %>% \n  mutate(perf = if_else(event == \"400\" & str_starts(perf, \"6\"), time_cleaner(perf), perf)) %>% \n  mutate(perf_x = case_when(event %in% c(\"800\", \"1500\") ~paste0(\"00:0\", perf),\n                              event %in% c(\"200\", \"400\") & nchar(perf) <= 5 ~paste0(\"00:00:\", perf), \n                              event == \"100\" ~ ifelse(nchar(perf) < 5, \n                                                      paste0(\"00:00:0\", perf), \n                                                      paste0(\"00:00:\", perf)),\n                              TRUE ~ paste0(\"00:\", perf))) %>%  \n  mutate(perf_x = parse_hms(perf_x),\n         perf_x_sec = round(seconds(perf_x), digits = 2),\n         perf_x_sec = round(seconds_to_period(perf_x_sec), 2)) %>% \n  mutate(time = as_datetime(perf_x_sec)) \n\n\n\n\nNumber of athletes on the rankings list\nFigure @ref(fig:simple-counts) shows the number of athletes on the ranking list for each event over the last five seasons.\n\n\n\n\n\nNumber of athletes on the rankings\n\n\n\n\nThere is a fairly similar trend for both men and women, although women‚Äôs 5K does stand out as being particularly low, compared to men‚Äôs 5k in previous seasons. Unsurprisingly, we also see a large drop off in the number of athletes on the rankings list in 2020, which is to be expected given the restrictions over the summer resulting in fewer opportunities to compete. There is a similar trend for both men and women with the middle distance events having the most performances, followed by the sprints events, with the 5000 m resulting in the fewest performances.\nTable @ref(tab:mytbl-4) compares the number of performances in 2019 and 2020 and highlights the percentage decrease between the two seasons. We see a similar trend for both men and women, with 5k performances suffering the greatest decrease (down 91% for men and 87% for women) and the 800 m with the smallest decrease (down 48% for men and 53% for women) out of the events studied.\n\nTable 1: The decrease in athletes on the ranking list for 2019 and 2020\n\n(#tab:mytbl-4)\n\n\nClick on column headers to sort each column\n\n\n\n\n\n\n\n\n\nDistribution of performances\nSo we already know that there have been far fewer performances this year, but of those performances that have been recorded, how to they compare to the kinds of performances we would see in a ‚Äúnormal‚Äù year?\nTo do this, I first calculated the average performance for each rank over the 2016 to 2019 seasons, and then compared the distribution of these average performances to those we‚Äôve seen this year. I‚Äôve coloured the average performance in this colour and highlighted 2020 performances in this colour. I have also split the rankings into two groups; ‚ÄúElite‚Äù, defined as the top 10 athletes and ‚ÄúSub elite‚Äù, defined as those athletes occupying ranks 11-100.\nThe y axis on the figure is the kernel density estimate, which is a smoothed version of the histogram. For these plots, I‚Äôve removed the y axis label as I wanted the focus to be on the shape of the density estimate and position relative the the x-axis (performance) for the two groups (average of previous years vs this year), rather than the actual density estimate as I think comparing the shape and position of the curves tells the better story.\n\n\nCode\nrank_plots <- clean_rankings %>% \n  mutate(input_year = fct_rev(input_year),\n         covid_year = ifelse(input_year == \"2020\", \"covid\", \"pre_covid\")) %>% \n  select(input_year, time, rank, event, gender, covid_year) %>% \n  group_by(gender, event, input_year) %>% \n  mutate(rank_group = case_when(rank %in% c(1:10) ~ \"top_10\",\n                                rank %in% c(11:100) ~ \"11_to_100\",\n                                rank %in% c(101:max(rank)) ~ \"101_to_max\"),\n         rank_group = factor(rank_group, levels = c(\"top_10\", \"11_to_100\", \"101_to_max\"))) %>% \n  ungroup() %>% \n  group_by(event, gender, covid_year, rank, rank_group) %>% \n  mutate(Performance = as_datetime(mean(as.POSIXct(time)))) %>% \n  ungroup() %>% \n  mutate(event = paste(event, \"m\"),\n         event = factor(event, levels = c(\"100 m\", \"200 m\", \"400 m\", \"800 m\", \"1500 m\", \"5000 m\")),\n         lab = \"wow\")\n\n\n\n\nElite performances\nFigure @ref(fig:elite-performances) shows the distribution of performances for the top 10 athletes. Interestingly we see that the sprint events (100, 200 and to a slightly lesser extent 400) seem to have slower performances in 2020, compared to the average over 2016-2019 (distributions shifted to the right indicated slower performances). In comparison, the middle distance events (800, 1500) seem to have equivalent (and in some cases even slightly better) performances in 2020 compared to average from previous season.\n\n\n\n\n\nElite performance distributions for males and females and different events.\n\n\n\n\n\n\nSub-elite performances\nFigure @ref(fig:sub-elite-perfs) is the same plot as above, but those those athletes ranked 11-100. We see a similar trend in the sprint events, however the middle distance events also show a shift to the right, indicating slower than average performances for these ranks in 2020.\n\n\n\n\n\nSub-elite performance distributions for males and females and different events."
  },
  {
    "objectID": "posts/2020-12-26-powerof10-covid/index.html#conclusions",
    "href": "posts/2020-12-26-powerof10-covid/index.html#conclusions",
    "title": "How has Covid-19 affected UK athletics rankings in 2020?",
    "section": "Conclusions",
    "text": "Conclusions\nFrom this small investigation there are a few key things that stood out to me. Firstly, elite middle distance runners had an excellent year (performance wise) in 2020. This was true for both men and women. In contrast, sprint events were considerably down on their average performances in 2020. My personal take on this is that middle distance runners may have faired better during periods of lockdown compared to sprinters who typically (I may be stereotyping here‚Ä¶!) make greater use of indoor facilities and therefore may have not been able to prepare as they normally would for the competitive season.\nFor sub-elite athletes (ranks 11 to 100), a consistent trend was apparent across all events, with performances in 2020 being down on the average performance over the last few seasons. This is probably due to a number of factors including, limited access to training facilities and far fewer opportunities to compete, not to mention all the other stresses that 2020 has brought.\nWhile this might all sound a little pessimistic, I know athletes are a hardy bunch and fully expect some exceptional performances in all events next year. The limited opportunities to compete this year, while certainly frustrating, may actually be a benefit in the long run‚Ä¶ After all, a rest is sometimes as good as a run, and maybe this extended break from intense competition will be a blessing in disguise!\nThanks for reading!"
  },
  {
    "objectID": "posts/2020-12-26-powerof10-covid/index.html#acknowledgments",
    "href": "posts/2020-12-26-powerof10-covid/index.html#acknowledgments",
    "title": "How has Covid-19 affected UK athletics rankings in 2020?",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nAs always, big thanks to the powerof10 team who provide an invaluable source for athletes and fans alike!"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Data Code & Coffee",
    "section": "",
    "text": "R packages\n\n  \n    \n      \n    \n    \n    \n        \n      \n      qbr\n      R interface to jquery QueryBuilder\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      poweRof10\n      Access data from athletics rankings database www.powerof10.info\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      bitmexr\n      API wrapper for www.bitmex.com\n   \n  \n\nShiny\n\n  \n    \n      \n    \n    \n    \n        \n      \n      shinyvuer\n      Demo of using Shiny and Vue together\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      qbr demo\n      Example of {qbr} in action.\n   \n  \n\n\nNo matching items"
  }
]